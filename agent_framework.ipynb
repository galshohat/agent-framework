{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321ecba2",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">üëã Welcome to the Microsoft Agent Framework Demo</h1>\n",
    "\n",
    "<h2 align=\"center\">üöÄ Microsoft Agent Framework ‚Äî Support Email Copilot (Capstone Demo)</h2>\n",
    "\n",
    "<p align=\"center\">In this notebook, we‚Äôll build a futuristic multi-agent Support Email Copilot using the Microsoft Agent Framework (Python SDK).</p>\n",
    "\n",
    "![main.png](images/main.png)\n",
    "\n",
    "\n",
    "<h1 align=\"center\">Microsoft Agent Framework</h1>\n",
    "\n",
    "![Microsoft Agent Framework](images/maf.png)\n",
    "\n",
    "The **Microsoft Agent Framework** is a Python SDK for building AI agents and multi-agent workflows. It provides a unified interface for creating intelligent systems that can reason, take actions, and collaborate.\n",
    "\n",
    "**Key Capabilities:**\n",
    "- Single and multi-agent orchestration\n",
    "- Tool integration and function calling\n",
    "- Memory and context management\n",
    "- Workflow patterns (sequential, parallel, branching)\n",
    "- Built-in observability and middleware\n",
    "\n",
    "---\n",
    "\n",
    "## What is an Agent?\n",
    "\n",
    "![What is an Agent](images/what-is-agent.png)\n",
    "\n",
    "Unlike traditional LLM deployments that simply respond to prompts, agents follow the **ReAct pattern** (Reasoning + Acting):\n",
    "\n",
    "| Traditional LLM | Agent (ReAct) |\n",
    "|-----------------|---------------|\n",
    "| Input ‚Üí Output | Input ‚Üí Reason ‚Üí Act ‚Üí Observe ‚Üí Repeat |\n",
    "| Single response | Multi-step execution |\n",
    "| No tool access | Tool integration |\n",
    "| Stateless | Memory & context |\n",
    "\n",
    "Agents autonomously decide *what* to do, *which* tools to use, and *when* to stop.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflows & Multi-Agent Orchestration\n",
    "\n",
    "![Workflow Example](images/workflow-example.png)\n",
    "\n",
    "Complex tasks require coordination between multiple specialized agents. The Agent Framework provides workflow primitives:\n",
    "\n",
    "- **Sequential** ‚Äî Agents execute in order (A ‚Üí B ‚Üí C)\n",
    "- **Parallel (Fan-out/Fan-in)** ‚Äî Concurrent execution with result aggregation\n",
    "- **Branching** ‚Äî Conditional routing based on outputs\n",
    "- **Group Chat** ‚Äî Collaborative multi-agent discussions\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "We'll build a **Support Email Copilot** that demonstrates core framework concepts:\n",
    "\n",
    "| Section | Concept |\n",
    "|---------|---------|\n",
    "| 1-2 | Agent basics & streaming |\n",
    "| 3-4 | Conversations & function tools |\n",
    "| 5-7 | Approvals, middleware, memory |\n",
    "| 8-10 | Workflows: sequential, branching, parallel |\n",
    "| 11-12 | Multi-agent collaboration & capstone |\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure subscription with Azure OpenAI access\n",
    "- Azure OpenAI resource with deployed model (e.g., `gpt-4o-mini`)\n",
    "- Azure CLI installed and authenticated (`az login`)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d841bbb",
   "metadata": {},
   "source": [
    "# 0. Environment Setup\n",
    "\n",
    "## Create Virtual Environment\n",
    "\n",
    "Run the following in your terminal to set up the environment:\n",
    "\n",
    "```bash\n",
    "python3.10 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Or run the cell below to install dependencies directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0f53fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found Python 3.13: /opt/homebrew/bin/python3.13\n",
      "Collecting agent-framework (from -r requirements.txt (line 4))\n",
      "  Using cached agent_framework-1.0.0b260130-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting python-dotenv>=1.0.0 (from -r requirements.txt (line 7))\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting nest-asyncio>=1.5.0 (from -r requirements.txt (line 10))\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting azure-identity>=1.15.0 (from -r requirements.txt (line 13))\n",
      "  Using cached azure_identity-1.26.0b1-py3-none-any.whl.metadata (88 kB)\n",
      "Collecting pydantic>=2.0.0 (from -r requirements.txt (line 16))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting agent-framework-core==1.0.0b260130 (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_core-1.0.0b260130-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pydantic-settings<3,>=2 (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting opentelemetry-api>=1.39.0 (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk>=1.39.0 (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai>=0.4.13 (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached opentelemetry_semantic_conventions_ai-0.4.13-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting openai>=1.99.0 (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached openai-2.16.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mcp<2,>=1.24.0 (from mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached mcp-1.26.0-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting packaging>=24.1 (from agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting agent-framework-a2a (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_a2a-1.0.0b260130-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting agent-framework-ag-ui (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_ag_ui-1.0.0b260130-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting agent-framework-azure-ai-search (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_azure_ai_search-1.0.0b260130-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting agent-framework-anthropic (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_anthropic-1.0.0b260130-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting agent-framework-azure-ai (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_azure_ai-1.0.0b260130-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting agent-framework-azurefunctions (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_azurefunctions-1.0.0b260130-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting agent-framework-chatkit (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_chatkit-1.0.0b260130-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting agent-framework-copilotstudio (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_copilotstudio-1.0.0b260130-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting agent-framework-declarative (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_declarative-1.0.0b260130-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting agent-framework-devui (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_devui-1.0.0b260130-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting agent-framework-durabletask (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_durabletask-1.0.0b260130-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting agent-framework-github-copilot (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_github_copilot-1.0.0b260130-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting agent-framework-lab (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_lab-1.0.0b251024-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting agent-framework-mem0 (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_mem0-1.0.0b260130-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting agent-framework-ollama (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_ollama-1.0.0b260130-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting agent-framework-purview (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_purview-1.0.0b260130-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting agent-framework-redis (from agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached agent_framework_redis-1.0.0b260130-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached azure_core-1.38.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Downloading cryptography-46.0.4-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached msal-1.35.0b1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->-r requirements.txt (line 16))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.0.0->-r requirements.txt (line 16))\n",
      "  Using cached pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.0.0->-r requirements.txt (line 16))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio>=4.5 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached httpx-1.0.dev3-py3-none-any.whl.metadata (646 bytes)\n",
      "Collecting jsonschema>=4.20.0 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pyjwt-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached python_multipart-0.0.22-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached sse_starlette-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting starlette>=0.27 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached starlette-0.52.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting uvicorn>=0.31.1 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting websockets>=15.0.1 (from mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading websockets-16.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting idna>=2.8 (from anyio>=4.5->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core>=1.31.0->azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=2.5->azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=2.5->azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting certifi (from httpx>=0.27.1->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.20.0->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.25.0 (from jsonschema>=4.20.0->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading rpds_py-0.30.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0->-r requirements.txt (line 13))\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.99.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.99.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading jiter-0.13.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.99.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>=1.99.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.1->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.39.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.39.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.39.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting click>=7.0 (from uvicorn>=0.31.1->mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting a2a-sdk>=0.3.5 (from agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached a2a_sdk-0.3.22-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting google-api-core>=1.26.0 (from a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached google_api_core-2.29.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf>=5.29.5 (from a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached protobuf-7.34.0rc1-cp310-abi3-macosx_10_9_universal2.whl.metadata (598 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core>=1.26.0->a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting protobuf>=5.29.5 (from a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached protobuf-6.33.5-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core>=1.26.0->a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading proto_plus-1.27.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-api-core>=1.26.0->a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached google_auth-2.49.0.dev0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-api-core>=1.26.0->a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core>=1.26.0->a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting ag-ui-protocol>=0.1.9 (from agent-framework-ag-ui->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached ag_ui_protocol-0.1.10-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fastapi>=0.115.0 (from agent-framework-ag-ui->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting starlette>=0.27 (from mcp<2,>=1.24.0->mcp[ws]<2,>=1.24.0->agent-framework-core==1.0.0b260130->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi>=0.115.0->agent-framework-ag-ui->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting anthropic<1,>=0.70.0 (from agent-framework-anthropic->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading anthropic-0.77.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic<1,>=0.70.0->agent-framework-anthropic->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting azure-ai-projects>=2.0.0b3 (from agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached azure_ai_projects-2.0.0b3-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting azure-ai-agents==1.2.0b5 (from agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached azure_ai_agents-1.2.0b5-py3-none-any.whl.metadata (74 kB)\n",
      "Collecting aiohttp (from agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-agents==1.2.0b5->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects>=2.0.0b3->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached azure_storage_blob-12.29.0b1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting azure-search-documents==11.7.0b2 (from agent-framework-azure-ai-search->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached azure_search_documents-11.7.0b2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting azure-common>=1.1 (from azure-search-documents==11.7.0b2->agent-framework-azure-ai-search->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting azure-functions (from agent-framework-azurefunctions->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading azure_functions-1.25.0b3.dev3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting azure-functions-durable (from agent-framework-azurefunctions->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached azure_functions_durable-1.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting openai-chatkit<2.0.0,>=1.4.0 (from agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached openai_chatkit-1.6.0-py3-none-any.whl.metadata (432 bytes)\n",
      "Collecting openai-agents>=0.3.2 (from openai-chatkit<2.0.0,>=1.4.0->agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached openai_agents-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jinja2<4,>=3.1 (from openai-chatkit<2.0.0,>=1.4.0->agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<4,>=3.1->openai-chatkit<2.0.0,>=1.4.0->agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting griffe<2,>=1.5.6 (from openai-agents>=0.3.2->openai-chatkit<2.0.0,>=1.4.0->agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting types-requests<3,>=2.0 (from openai-agents>=0.3.2->openai-chatkit<2.0.0,>=1.4.0->agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached types_requests-2.32.4.20260107-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents>=0.3.2->openai-chatkit<2.0.0,>=1.4.0->agent-framework-chatkit->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting microsoft-agents-copilotstudio-client>=0.3.1 (from agent-framework-copilotstudio->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached microsoft_agents_copilotstudio_client-0.7.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting microsoft-agents-hosting-core==0.7.0 (from microsoft-agents-copilotstudio-client>=0.3.1->agent-framework-copilotstudio->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached microsoft_agents_hosting_core-0.7.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting microsoft-agents-activity==0.7.0 (from microsoft-agents-hosting-core==0.7.0->microsoft-agents-copilotstudio-client>=0.3.1->agent-framework-copilotstudio->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached microsoft_agents_activity-0.7.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting powerfx>=0.0.31 (from agent-framework-declarative->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached powerfx-0.0.34-py3-none-any.whl.metadata (795 bytes)\n",
      "Collecting pyyaml<7.0,>=6.0 (from agent-framework-declarative->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting pythonnet==3.0.5 (from powerfx>=0.0.31->agent-framework-declarative->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pythonnet-3.0.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting clr_loader<0.3.0,>=0.2.7 (from pythonnet==3.0.5->powerfx>=0.0.31->agent-framework-declarative->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached clr_loader-0.2.10-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.24.0->agent-framework-devui->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading httptools-0.7.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.5 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.24.0->agent-framework-devui->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading uvloop-0.22.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.24.0->agent-framework-devui->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading watchfiles-1.1.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting durabletask>=1.3.0 (from agent-framework-durabletask->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached durabletask-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting durabletask-azuremanaged>=1.3.0 (from agent-framework-durabletask->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached durabletask_azuremanaged-1.3.0-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting python-dateutil>=2.8.0 (from agent-framework-durabletask->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting grpcio (from durabletask>=1.3.0->agent-framework-durabletask->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading grpcio-1.78.0rc2-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting asyncio (from durabletask>=1.3.0->agent-framework-durabletask->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached asyncio-4.0.0-py3-none-any.whl.metadata (994 bytes)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.0->agent-framework-durabletask->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting github-copilot-sdk>=0.1.0 (from agent-framework-github-copilot->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached github_copilot_sdk-0.1.20-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting mem0ai>=1.0.0 (from agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading mem0ai-1.0.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting posthog>=3.5.0 (from mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached posthog-7.8.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting protobuf>=5.29.5 (from a2a-sdk>=0.3.5->agent-framework-a2a->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pytz>=2024.1 (from mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting qdrant-client>=1.9.1 (from mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached qdrant_client-1.16.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sqlalchemy>=2.0.31 (from mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading sqlalchemy-2.1.0b1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=3.5.0->mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=2.1.0 (from qdrant-client>=1.9.1->mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading numpy-2.4.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client>=1.9.1->mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai>=1.0.0->agent-framework-mem0->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting ollama>=0.5.3 (from agent-framework-ollama->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting redis>=6.4.0 (from agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached redis-7.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting redisvl>=0.8.2 (from agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached redisvl-0.13.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting jsonpath-ng>=1.5.0 (from redisvl>=0.8.2->agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.4.0 (from redisvl>=0.8.2->agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading ml_dtypes-0.5.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (8.9 kB)\n",
      "Collecting python-ulid>=3.0.0 (from redisvl>=0.8.2->agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached python_ulid-3.1.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tenacity>=8.2.2 (from redisvl>=0.8.2->agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting ply (from jsonpath-ng>=1.5.0->redisvl>=0.8.2->agent-framework-redis->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.7.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->agent-framework-azure-ai->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting werkzeug~=3.1.3 (from azure-functions->agent-framework-azurefunctions->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting furl>=2.1.0 (from azure-functions-durable->agent-framework-azurefunctions->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\n",
      "Collecting orderedmultidict>=1.0.1 (from furl>=2.1.0->azure-functions-durable->agent-framework-azurefunctions->agent-framework-core[all]==1.0.0b260130->agent-framework->-r requirements.txt (line 4))\n",
      "  Using cached orderedmultidict-1.0.2-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Using cached agent_framework-1.0.0b260130-py3-none-any.whl (5.6 kB)\n",
      "Using cached agent_framework_core-1.0.0b260130-py3-none-any.whl (348 kB)\n",
      "Using cached azure_identity-1.26.0b1-py3-none-any.whl (197 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached mcp-1.26.0-py3-none-any.whl (233 kB)\n",
      "Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached azure_core-1.38.0-py3-none-any.whl (217 kB)\n",
      "Downloading cryptography-46.0.4-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl (181 kB)\n",
      "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached msal-1.35.0b1-py3-none-any.whl (117 kB)\n",
      "Using cached pyjwt-2.11.0-py3-none-any.whl (28 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Using cached openai-2.16.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.13.0-cp313-cp313-macosx_11_0_arm64.whl (317 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Using cached importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Using cached opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Using cached opentelemetry_semantic_conventions_ai-0.4.13-py3-none-any.whl (6.1 kB)\n",
      "Using cached packaging-26.0-py3-none-any.whl (74 kB)\n",
      "Using cached python_multipart-0.0.22-py3-none-any.whl (24 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.30.0-cp313-cp313-macosx_11_0_arm64.whl (358 kB)\n",
      "Using cached sse_starlette-3.2.0-py3-none-any.whl (12 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading websockets-16.0-cp313-cp313-macosx_11_0_arm64.whl (175 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached agent_framework_a2a-1.0.0b260130-py3-none-any.whl (7.5 kB)\n",
      "Using cached a2a_sdk-0.3.22-py3-none-any.whl (144 kB)\n",
      "Using cached google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
      "Using cached google_auth-2.49.0.dev0-py3-none-any.whl (236 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading proto_plus-1.27.1-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Using cached agent_framework_ag_ui-1.0.0b260130-py3-none-any.whl (67 kB)\n",
      "Using cached ag_ui_protocol-0.1.10-py3-none-any.whl (7.9 kB)\n",
      "Using cached fastapi-0.128.0-py3-none-any.whl (103 kB)\n",
      "Using cached starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Using cached annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached agent_framework_anthropic-1.0.0b260130-py3-none-any.whl (12 kB)\n",
      "Downloading anthropic-0.77.1-py3-none-any.whl (397 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached agent_framework_azure_ai-1.0.0b260130-py3-none-any.whl (38 kB)\n",
      "Using cached azure_ai_agents-1.2.0b5-py3-none-any.whl (217 kB)\n",
      "Using cached azure_ai_projects-2.0.0b3-py3-none-any.whl (240 kB)\n",
      "Using cached azure_storage_blob-12.29.0b1-py3-none-any.whl (434 kB)\n",
      "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Using cached agent_framework_azure_ai_search-1.0.0b260130-py3-none-any.whl (13 kB)\n",
      "Using cached azure_search_documents-11.7.0b2-py3-none-any.whl (432 kB)\n",
      "Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Using cached agent_framework_azurefunctions-1.0.0b260130-py3-none-any.whl (17 kB)\n",
      "Using cached agent_framework_chatkit-1.0.0b260130-py3-none-any.whl (11 kB)\n",
      "Using cached openai_chatkit-1.6.0-py3-none-any.whl (42 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached openai_agents-0.7.0-py3-none-any.whl (288 kB)\n",
      "Using cached griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Using cached types_requests-2.32.4.20260107-py3-none-any.whl (20 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached agent_framework_copilotstudio-1.0.0b260130-py3-none-any.whl (8.7 kB)\n",
      "Using cached microsoft_agents_copilotstudio_client-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached microsoft_agents_hosting_core-0.7.0-py3-none-any.whl (133 kB)\n",
      "Using cached microsoft_agents_activity-0.7.0-py3-none-any.whl (132 kB)\n",
      "Using cached agent_framework_declarative-1.0.0b260130-py3-none-any.whl (89 kB)\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached powerfx-0.0.34-py3-none-any.whl (3.5 MB)\n",
      "Using cached pythonnet-3.0.5-py3-none-any.whl (297 kB)\n",
      "Using cached clr_loader-0.2.10-py3-none-any.whl (56 kB)\n",
      "Using cached agent_framework_devui-1.0.0b260130-py3-none-any.whl (359 kB)\n",
      "Downloading httptools-0.7.1-cp313-cp313-macosx_11_0_arm64.whl (108 kB)\n",
      "Downloading uvloop-0.22.1-cp313-cp313-macosx_10_13_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.1-cp313-cp313-macosx_11_0_arm64.whl (391 kB)\n",
      "Using cached agent_framework_durabletask-1.0.0b260130-py3-none-any.whl (36 kB)\n",
      "Using cached durabletask-1.3.0-py3-none-any.whl (64 kB)\n",
      "Using cached durabletask_azuremanaged-1.3.0-py3-none-any.whl (6.4 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached agent_framework_github_copilot-1.0.0b260130-py3-none-any.whl (8.8 kB)\n",
      "Using cached github_copilot_sdk-0.1.20-py3-none-any.whl (40 kB)\n",
      "Using cached agent_framework_lab-1.0.0b251024-py3-none-any.whl (26 kB)\n",
      "Using cached agent_framework_mem0-1.0.0b260130-py3-none-any.whl (5.6 kB)\n",
      "Downloading mem0ai-1.0.3-py3-none-any.whl (275 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Using cached posthog-7.8.0-py3-none-any.whl (192 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached qdrant_client-1.16.2-py3-none-any.whl (377 kB)\n",
      "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading grpcio-1.78.0rc2-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading numpy-2.4.2-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.1.0b1-cp313-cp313-macosx_11_0_arm64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached agent_framework_ollama-1.0.0b260130-py3-none-any.whl (9.2 kB)\n",
      "Using cached ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Using cached agent_framework_purview-1.0.0b260130-py3-none-any.whl (26 kB)\n",
      "Using cached agent_framework_redis-1.0.0b260130-py3-none-any.whl (16 kB)\n",
      "Using cached redis-7.1.0-py3-none-any.whl (354 kB)\n",
      "Using cached redisvl-0.13.2-py3-none-any.whl (192 kB)\n",
      "Downloading ml_dtypes-0.5.4-cp313-cp313-macosx_10_13_universal2.whl (676 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m676.9/676.9 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n",
      "Using cached python_ulid-3.1.0-py3-none-any.whl (11 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl (490 kB)\n",
      "Downloading multidict-6.7.1-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached asyncio-4.0.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading azure_functions-1.25.0b3.dev3-py3-none-any.whl (114 kB)\n",
      "Using cached werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Using cached azure_functions_durable-1.4.0-py3-none-any.whl (143 kB)\n",
      "Using cached furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
      "Using cached orderedmultidict-1.0.2-py2.py3-none-any.whl (11 kB)\n",
      "Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Using cached pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pytz, ply, azure-common, zipp, websockets, uvloop, urllib3, typing-extensions, tqdm, tenacity, sniffio, six, rpds-py, redis, pyyaml, python-ulid, python-multipart, python-dotenv, pyjwt, pycparser, pyasn1, protobuf, propcache, portalocker, packaging, opentelemetry-semantic-conventions-ai, numpy, nest-asyncio, multidict, MarkupSafe, jsonpath-ng, jiter, isodate, idna, hyperframe, httpx-sse, httptools, hpack, h11, frozenlist, docstring-parser, distro, colorama, click, charset_normalizer, certifi, backoff, attrs, asyncio, annotated-types, annotated-doc, aiohappyeyeballs, yarl, werkzeug, uvicorn, typing-inspection, types-requests, sqlalchemy, requests, referencing, python-dateutil, pydantic-core, pyasn1-modules, proto-plus, orderedmultidict, ml-dtypes, jinja2, importlib-metadata, httpcore, h2, grpcio, griffe, googleapis-common-protos, cffi, anyio, aiosignal, watchfiles, starlette, pydantic, posthog, opentelemetry-api, jsonschema-specifications, httpx, furl, durabletask, cryptography, clr_loader, azure-functions, azure-core, aiohttp, sse-starlette, redisvl, pythonnet, pydantic-settings, opentelemetry-semantic-conventions, openai, ollama, microsoft-agents-activity, jsonschema, google-auth, github-copilot-sdk, fastapi, azure-storage-blob, azure-search-documents, azure-ai-agents, anthropic, ag-ui-protocol, qdrant-client, powerfx, opentelemetry-sdk, msal, microsoft-agents-hosting-core, mcp, google-api-core, openai-agents, msal-extensions, microsoft-agents-copilotstudio-client, mem0ai, azure-functions-durable, a2a-sdk, openai-chatkit, azure-identity, durabletask-azuremanaged, azure-ai-projects, agent-framework-core, agent-framework-redis, agent-framework-purview, agent-framework-ollama, agent-framework-mem0, agent-framework-lab, agent-framework-github-copilot, agent-framework-durabletask, agent-framework-devui, agent-framework-declarative, agent-framework-copilotstudio, agent-framework-chatkit, agent-framework-azure-ai-search, agent-framework-azure-ai, agent-framework-anthropic, agent-framework-ag-ui, agent-framework-a2a, agent-framework-azurefunctions, agent-framework\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143/143\u001b[0m [agent-framework][agent-framework-declarative]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 a2a-sdk-0.3.22 ag-ui-protocol-0.1.10 agent-framework-1.0.0b260130 agent-framework-a2a-1.0.0b260130 agent-framework-ag-ui-1.0.0b260130 agent-framework-anthropic-1.0.0b260130 agent-framework-azure-ai-1.0.0b260130 agent-framework-azure-ai-search-1.0.0b260130 agent-framework-azurefunctions-1.0.0b260130 agent-framework-chatkit-1.0.0b260130 agent-framework-copilotstudio-1.0.0b260130 agent-framework-core-1.0.0b260130 agent-framework-declarative-1.0.0b260130 agent-framework-devui-1.0.0b260130 agent-framework-durabletask-1.0.0b260130 agent-framework-github-copilot-1.0.0b260130 agent-framework-lab-1.0.0b251024 agent-framework-mem0-1.0.0b260130 agent-framework-ollama-1.0.0b260130 agent-framework-purview-1.0.0b260130 agent-framework-redis-1.0.0b260130 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 annotated-doc-0.0.4 annotated-types-0.7.0 anthropic-0.77.1 anyio-4.12.1 asyncio-4.0.0 attrs-25.4.0 azure-ai-agents-1.2.0b5 azure-ai-projects-2.0.0b3 azure-common-1.1.28 azure-core-1.38.0 azure-functions-1.25.0b3.dev3 azure-functions-durable-1.4.0 azure-identity-1.26.0b1 azure-search-documents-11.7.0b2 azure-storage-blob-12.29.0b1 backoff-2.2.1 certifi-2026.1.4 cffi-2.0.0 charset_normalizer-3.4.4 click-8.3.1 clr_loader-0.2.10 colorama-0.4.6 cryptography-46.0.4 distro-1.9.0 docstring-parser-0.17.0 durabletask-1.3.0 durabletask-azuremanaged-1.3.0 fastapi-0.128.0 frozenlist-1.8.0 furl-2.1.4 github-copilot-sdk-0.1.20 google-api-core-2.29.0 google-auth-2.49.0.dev0 googleapis-common-protos-1.72.0 griffe-1.15.0 grpcio-1.78.0rc2 h11-0.16.0 h2-4.3.0 hpack-4.1.0 httpcore-1.0.9 httptools-0.7.1 httpx-0.28.1 httpx-sse-0.4.3 hyperframe-6.1.0 idna-3.11 importlib-metadata-8.7.1 isodate-0.7.2 jinja2-3.1.6 jiter-0.13.0 jsonpath-ng-1.7.0 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 mcp-1.26.0 mem0ai-1.0.3 microsoft-agents-activity-0.7.0 microsoft-agents-copilotstudio-client-0.7.0 microsoft-agents-hosting-core-0.7.0 ml-dtypes-0.5.4 msal-1.35.0b1 msal-extensions-1.3.1 multidict-6.7.1 nest-asyncio-1.6.0 numpy-2.4.2 ollama-0.6.1 openai-2.16.0 openai-agents-0.7.0 openai-chatkit-1.6.0 opentelemetry-api-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 opentelemetry-semantic-conventions-ai-0.4.13 orderedmultidict-1.0.2 packaging-26.0 ply-3.11 portalocker-3.2.0 posthog-7.8.0 powerfx-0.0.34 propcache-0.4.1 proto-plus-1.27.1 protobuf-5.29.5 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 pyjwt-2.11.0 python-dateutil-2.9.0.post0 python-dotenv-1.2.1 python-multipart-0.0.22 python-ulid-3.1.0 pythonnet-3.0.5 pytz-2025.2 pyyaml-6.0.3 qdrant-client-1.16.2 redis-7.1.0 redisvl-0.13.2 referencing-0.37.0 requests-2.32.5 rpds-py-0.30.0 six-1.17.0 sniffio-1.3.1 sqlalchemy-2.1.0b1 sse-starlette-3.2.0 starlette-0.50.0 tenacity-9.1.2 tqdm-4.67.3 types-requests-2.32.4.20260107 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.6.3 uvicorn-0.40.0 uvloop-0.22.1 watchfiles-1.1.1 websockets-16.0 werkzeug-3.1.5 yarl-1.22.0 zipp-3.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Virtual environment created at .venv\n",
      "   Activate with: source .venv/bin/activate\n"
     ]
    }
   ],
   "source": [
    "# Create and configure the virtual environment (run once)\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def find_python():\n",
    "    \"\"\"Find a Python 3.10+ interpreter on the system.\"\"\"\n",
    "    # Check common Python commands in order of preference\n",
    "    candidates = [\n",
    "        \"python3.13\", \"python3.12\", \"python3.11\", \"python3.10\",\n",
    "        \"python3\", \"python\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in candidates:\n",
    "        path = shutil.which(cmd)\n",
    "        if path:\n",
    "            # Verify version is 3.10+\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [path, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"],\n",
    "                    capture_output=True, text=True\n",
    "                )\n",
    "                version = result.stdout.strip()\n",
    "                major, minor = map(int, version.split('.'))\n",
    "                if major >= 3 and minor >= 10:\n",
    "                    return path, version\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    raise RuntimeError(\"No Python 3.10+ found. Please install Python 3.10 or higher.\")\n",
    "\n",
    "# Find suitable Python\n",
    "python_path, python_version = find_python()\n",
    "print(f\"‚úÖ Found Python {python_version}: {python_path}\")\n",
    "\n",
    "# Create .venv\n",
    "subprocess.run([python_path, \"-m\", \"venv\", \".venv\"])\n",
    "\n",
    "# Install requirements with pre-release flag\n",
    "subprocess.run([\".venv/bin/pip\", \"install\", \"-r\", \"requirements.txt\", \"--pre\"])\n",
    "\n",
    "print(\"\\n‚úÖ Virtual environment created at .venv\")\n",
    "print(\"   Activate with: source .venv/bin/activate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e423b",
   "metadata": {},
   "source": [
    "## Initialize Chat Client\n",
    "\n",
    "Load environment variables and create the Azure OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d946629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded and chat_client created\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import AzureCliCredential\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create ONE chat client - reused throughout the notebook\n",
    "chat_client = AzureOpenAIChatClient(credential=AzureCliCredential())\n",
    "\n",
    "print(\"‚úÖ Environment loaded and chat_client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10492593",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "\n",
    "Pydantic models for structured input/output throughout the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e398ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Shared models defined: EmailInput, ClassificationResult, DraftResponse, FinalResponse\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# === Input Model ===\n",
    "class EmailInput(BaseModel):\n",
    "    \"\"\"Incoming support email.\"\"\"\n",
    "    sender: str = Field(description=\"Email sender address\")\n",
    "    subject: str = Field(description=\"Email subject line\")\n",
    "    body: str = Field(description=\"Email body content\")\n",
    "    customer_id: str | None = Field(default=None, description=\"Customer ID if known\")\n",
    "    ticket_id: str | None = Field(default=None, description=\"Related ticket ID if any\")\n",
    "\n",
    "# === Classification Model ===\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"Result of email classification.\"\"\"\n",
    "    category: Literal[\"spam\", \"not_spam\", \"uncertain\"] = Field(description=\"Email category\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n",
    "    reason: str = Field(description=\"Brief explanation of classification\")\n",
    "\n",
    "# === Draft Response Model ===\n",
    "class DraftResponse(BaseModel):\n",
    "    \"\"\"Draft reply to customer email.\"\"\"\n",
    "    subject: str = Field(description=\"Reply subject line\")\n",
    "    body: str = Field(description=\"Reply body\")\n",
    "    tone: Literal[\"formal\", \"friendly\", \"apologetic\"] = Field(description=\"Tone used\")\n",
    "    needs_review: bool = Field(default=False, description=\"Flag if needs human review\")\n",
    "\n",
    "# === Final Response Model ===\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"Final approved response.\"\"\"\n",
    "    classification: ClassificationResult\n",
    "    draft: DraftResponse | None = Field(default=None, description=\"Draft if not spam\")\n",
    "    review_notes: str | None = Field(default=None, description=\"Reviewer comments\")\n",
    "    approved: bool = Field(default=False, description=\"Whether approved to send\")\n",
    "\n",
    "print(\"‚úÖ Shared models defined: EmailInput, ClassificationResult, DraftResponse, FinalResponse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca66c9",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Test emails used throughout the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d68e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample emails defined: LEGIT_EMAIL, SPAM_EMAIL, AMBIGUOUS_EMAIL\n"
     ]
    }
   ],
   "source": [
    "# === LEGITIMATE EMAIL ===\n",
    "LEGIT_EMAIL = EmailInput(\n",
    "    sender=\"sarah.chen@acmecorp.com\",\n",
    "    subject=\"Order #12345 - Delivery Issue\",\n",
    "    body=\"\"\"Hi Support Team,\n",
    "\n",
    "I placed order #12345 last week and the tracking shows it was delivered, \n",
    "but I never received the package. I've checked with my neighbors and the building \n",
    "concierge, but no one has seen it.\n",
    "\n",
    "This is urgent as the items were needed for a client presentation on Friday.\n",
    "Can you please help me locate the package or arrange a replacement?\n",
    "\n",
    "Thank you,\n",
    "Sarah Chen\n",
    "Account: ACME-7891\n",
    "\"\"\",\n",
    "    customer_id=\"CUST-7891\",\n",
    "    ticket_id=\"TKT-2024-001\"\n",
    ")\n",
    "\n",
    "# === SPAM EMAIL ===\n",
    "SPAM_EMAIL = EmailInput(\n",
    "    sender=\"winner@prize-notifications.biz\",\n",
    "    subject=\"üéâ CONGRATULATIONS! You've WON $1,000,000!!!\",\n",
    "    body=\"\"\"URGENT NOTIFICATION!!!\n",
    "\n",
    "You have been selected as the WINNER of our international lottery!\n",
    "To claim your $1,000,000 prize, simply send your bank details and \n",
    "a processing fee of $500 to unlock your winnings.\n",
    "\n",
    "ACT NOW - This offer expires in 24 HOURS!!!\n",
    "\n",
    "Click here to claim: http://totally-legit-prize.com/claim\n",
    "\"\"\",\n",
    "    customer_id=None,\n",
    "    ticket_id=None\n",
    ")\n",
    "\n",
    "# === AMBIGUOUS EMAIL ===\n",
    "AMBIGUOUS_EMAIL = EmailInput(\n",
    "    sender=\"j.smith@unknown-domain.net\",\n",
    "    subject=\"Partnership Opportunity\",\n",
    "    body=\"\"\"Hello,\n",
    "\n",
    "I found your company online and I'm interested in discussing a potential \n",
    "business partnership. We have a new product line that might complement your services.\n",
    "\n",
    "Can we schedule a call this week?\n",
    "\n",
    "Best,\n",
    "J. Smith\n",
    "\"\"\",\n",
    "    customer_id=None,\n",
    "    ticket_id=None\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample emails defined: LEGIT_EMAIL, SPAM_EMAIL, AMBIGUOUS_EMAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26154c1d",
   "metadata": {},
   "source": [
    "# 1. Basic Agent\n",
    "\n",
    "![Agent Components](images/agent-components.png)\n",
    "\n",
    "Create a support agent using `chat_client.as_agent()` with instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f96ea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ support_agent created\n"
     ]
    }
   ],
   "source": [
    "# Create the core Support Agent - we'll enhance this throughout the notebook\n",
    "support_agent = chat_client.as_agent(\n",
    "    name=\"SupportAgent\",\n",
    "    instructions=\"\"\"You are a helpful customer support agent for an e-commerce company.\n",
    "Your job is to:\n",
    "1. Understand customer issues from their emails\n",
    "2. Draft professional, empathetic responses\n",
    "3. Provide clear next steps when possible\n",
    "\n",
    "Always be polite, acknowledge the customer's frustration, and offer concrete solutions.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ support_agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f5e7f",
   "metadata": {},
   "source": [
    "## Run the Agent\n",
    "\n",
    "Execute the agent with `agent.run()`. Returns an `AgentResponse` with `.text` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0324c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Draft Response:\n",
      "\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "Thank you for reaching out and sharing your concern regarding your recent order. I completely understand how frustrating it must be to have the tracking indicate that your package was delivered, yet not receiving it, especially with an important client presentation approaching.\n",
      "\n",
      "To address this issue, I will start by initiating a trace with our shipping partner to gather more information about your package's delivery. This process typically takes a couple of days, and I will keep you updated on any developments as we move forward.\n",
      "\n",
      "In the meantime, I would like to offer you the option of sending a replacement for the items you ordered. If you prefer this solution, please let me know, and I can expedite a new shipment to ensure you receive it in time for your presentation.\n",
      "\n",
      "Again, I apologize for the inconvenience caused, and I appreciate your understanding as we work to resolve this matter promptly. Please feel free to reach out if you have any further questions or concerns.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "Customer Support Team  \n",
      "[Your Company]  \n",
      "[Your Contact Information]  \n"
     ]
    }
   ],
   "source": [
    "# Run the support agent on our legitimate email\n",
    "async def run_basic_agent():\n",
    "    prompt = f\"\"\"Please draft a response to this customer email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    result = await support_agent.run(prompt)\n",
    "    print(\"üìß Draft Response:\\n\")\n",
    "    print(result.text)\n",
    "\n",
    "asyncio.run(run_basic_agent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9e31c",
   "metadata": {},
   "source": [
    "# 2. Streaming Responses\n",
    "\n",
    "Stream responses token-by-token using `agent.run_stream()` for real-time output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b03aef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Streaming Draft Response:\n",
      "\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "Thank you for reaching out to us regarding your order #12345. I sincerely apologize for the inconvenience and frustration this situation has caused, especially with your client presentation approaching.\n",
      "\n",
      "I understand how important it is to receive your items on time, and I‚Äôm here to help resolve this as quickly as possible. We will initiate an investigation with our shipping carrier to track the delivery of your package and determine what might have happened. \n",
      "\n",
      "In the meantime, could you please confirm if there are any specific details regarding your address that we should be aware of or if there are any alternate locations where the package could have been left? This information may assist us in locating your order more efficiently.\n",
      "\n",
      "If the investigation reveals that the package cannot be located, we can certainly arrange a replacement for you. I will keep you updated throughout the process.\n",
      "\n",
      "Thank you for your patience and understanding. We are committed to ensuring you receive your items in time for your presentation.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "Customer Support Team  \n",
      "[Your Company]  \n",
      "[Your Contact Information]\n"
     ]
    }
   ],
   "source": [
    "### Stream the response token by token using the SAME support_agent\n",
    "async def stream_support_response():\n",
    "    prompt = f\"\"\"Please draft a response to this customer email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    print(\"üìß Streaming Draft Response:\\n\")\n",
    "    async for update in support_agent.run_stream(prompt):\n",
    "        if update.text:\n",
    "            print(update.text, end=\"\", flush=True)\n",
    "    print()  # New line after streaming\n",
    "\n",
    "asyncio.run(stream_support_response())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54494e5c",
   "metadata": {},
   "source": [
    "# 3. Multi-Turn Conversations\n",
    "\n",
    "![Threads and Memory](images/threads-and-memory.png)\n",
    "\n",
    "Agents are stateless by default. Use **Threads** to maintain conversation context across turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4aa94",
   "metadata": {},
   "source": [
    "## Using Threads\n",
    "\n",
    "Create a thread with `agent.get_new_thread()` and pass it to each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bc87f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: Summarize the issue\n",
      "--------------------------------------------------\n",
      "- Sarah's order #12345 was marked as delivered, but she has not received the package.\n",
      "- She has checked with neighbors and the building concierge, but the package is still missing.\n",
      "- She needs urgent assistance to locate the package or arrange for a replacement, as the items are needed for a client presentation on Friday.\n",
      "\n",
      "Turn 2: Draft response with professional tone\n",
      "--------------------------------------------------\n",
      "Subject: Assistance with Your Order #12345\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "Thank you for reaching out to us regarding your order #12345. I understand how important it is to receive your items, especially with an upcoming client presentation on Friday. I sincerely apologize for the inconvenience you are experiencing.\n",
      "\n",
      "I appreciate you taking the time to check with your neighbors and building concierge. As the package shows as delivered, we will initiate an investigation with our shipping carrier to locate your missing package. This process can take a little time, but we will keep you updated with any information we receive.\n",
      "\n",
      "In the meantime, I would like to explore the option of arranging a replacement for you. If the investigation does not yield results, we want to ensure you have the items in time for your presentation. Please let me know if you would like us to proceed with this option or if you prefer to wait for updates on the original shipment.\n",
      "\n",
      "Thank you for your understanding and patience as we work to resolve this issue. Please do not hesitate to reach out if you have any further questions or concerns.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "Customer Support Team  \n"
     ]
    }
   ],
   "source": [
    "# Create a thread for multi-turn conversation\n",
    "thread = support_agent.get_new_thread()\n",
    "\n",
    "# Turn 1: Summarize the customer issue\n",
    "print(\"Turn 1: Summarize the issue\")\n",
    "print(\"-\" * 50)\n",
    "result1 = await support_agent.run(\n",
    "    f\"Summarize the key issues in this email in 2-3 bullet points:\\n\\n{LEGIT_EMAIL.body}\", \n",
    "    thread=thread\n",
    ")\n",
    "print(result1.text)\n",
    "print()\n",
    "\n",
    "# Turn 2: Draft a response (agent remembers the summary from Turn 1)\n",
    "print(\"Turn 2: Draft response with professional tone\")\n",
    "print(\"-\" * 50)\n",
    "result2 = await support_agent.run(\n",
    "    \"Now draft a professional response addressing each of those issues. Use a formal but empathetic tone.\",\n",
    "    thread=thread\n",
    ")\n",
    "print(result2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d5c10",
   "metadata": {},
   "source": [
    "# 4. Function Tools\n",
    "\n",
    "Extend agent capabilities by registering Python functions as tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d70fa",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "Use the `@tool` decorator to expose functions to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963debc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Support tools defined: lookup_customer_sla, get_incident_status\n"
     ]
    }
   ],
   "source": [
    "from agent_framework import tool\n",
    "# Simulated database of customer SLAs\n",
    "CUSTOMER_SLAS = {\n",
    "    \"CUST-7891\": {\"tier\": \"Premium\", \"response_time\": \"4 hours\", \"replacement_policy\": \"Free expedited replacement\"},\n",
    "    \"CUST-1234\": {\"tier\": \"Standard\", \"response_time\": \"24 hours\", \"replacement_policy\": \"Standard replacement\"},\n",
    "}\n",
    "\n",
    "# Simulated ticket database\n",
    "TICKET_STATUSES = {\n",
    "    \"TKT-2024-001\": {\"status\": \"Open\", \"priority\": \"High\", \"assigned_to\": \"Support Team\", \"last_update\": \"2024-01-15\"},\n",
    "    \"TKT-2024-002\": {\"status\": \"Resolved\", \"priority\": \"Low\", \"assigned_to\": \"Bot\", \"last_update\": \"2024-01-10\"},\n",
    "}\n",
    "\n",
    "@tool(name=\"lookup_customer_sla\", description=\"Look up a customer's SLA tier and policies\")\n",
    "def lookup_customer_sla(\n",
    "    customer_id: Annotated[str, Field(description=\"The customer ID to look up (e.g., CUST-7891)\")]\n",
    ") -> str:\n",
    "    \"\"\"Look up customer SLA information.\"\"\"\n",
    "    if customer_id in CUSTOMER_SLAS:\n",
    "        sla = CUSTOMER_SLAS[customer_id]\n",
    "        return f\"Customer {customer_id}: {sla['tier']} tier, {sla['response_time']} response time, {sla['replacement_policy']}\"\n",
    "    return f\"Customer {customer_id} not found in system.\"\n",
    "\n",
    "@tool(name=\"get_incident_status\", description=\"Get the current status of a support ticket\")\n",
    "def get_incident_status(\n",
    "    ticket_id: Annotated[str, Field(description=\"The ticket ID to check (e.g., TKT-2024-001)\")]\n",
    ") -> str:\n",
    "    \"\"\"Get ticket status information.\"\"\"\n",
    "    if ticket_id in TICKET_STATUSES:\n",
    "        ticket = TICKET_STATUSES[ticket_id]\n",
    "        return f\"Ticket {ticket_id}: Status={ticket['status']}, Priority={ticket['priority']}, Assigned to={ticket['assigned_to']}, Last update={ticket['last_update']}\"\n",
    "    return f\"Ticket {ticket_id} not found in system.\"\n",
    "\n",
    "print(\"‚úÖ Support tools defined: lookup_customer_sla, get_incident_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ee9eb",
   "metadata": {},
   "source": [
    "## Attach Tools to Agent\n",
    "\n",
    "Pass tools when creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363e4277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ support_agent_with_tools created\n"
     ]
    }
   ],
   "source": [
    "# Create support agent with tools\n",
    "support_agent_with_tools = chat_client.as_agent(\n",
    "    name=\"SupportAgentWithTools\",\n",
    "    instructions=\"\"\"You are a customer support agent with access to internal systems.\n",
    "When handling emails:\n",
    "1. Look up the customer's SLA tier to understand their service level\n",
    "2. Check ticket status if a ticket ID is mentioned\n",
    "3. Use this information to provide appropriate responses and set expectations\n",
    "\n",
    "Always be empathetic and use the customer's SLA tier to guide your response (e.g., Premium customers get expedited service).\"\"\",\n",
    "    tools=[lookup_customer_sla, get_incident_status]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ support_agent_with_tools created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461f75e",
   "metadata": {},
   "source": [
    "## Execute with Tools\n",
    "\n",
    "The agent autonomously decides when to invoke tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63cff641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Response (with tool lookups):\n",
      "\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "\n",
      "Hi Sarah,\n",
      "\n",
      "Thank you for reaching out, and I‚Äôm sorry to hear about the trouble with your delivery. I understand how important it is to have the items for your client presentation on Friday.\n",
      "\n",
      "I‚Äôve checked your account, and I see that you are a Premium customer, which means you can expect a response within 4 hours, and we also offer expedited replacements if necessary.\n",
      "\n",
      "Your ticket (TKT-2024-001) is currently open and has a high priority. Our support team is actively working on it. I will escalate your situation and ensure that we do everything possible to locate your package or arrange for a replacement promptly.\n",
      "\n",
      "In the meantime, if there are any more details you can provide or if you have further questions, please let me know.\n",
      "\n",
      "Thank you for your patience!\n",
      "\n",
      "Best regards,  \n",
      "[Your Name]  \n",
      "Customer Support Team  \n"
     ]
    }
   ],
   "source": [
    "# Test with the legitimate email that has customer_id and ticket_id\n",
    "prompt = f\"\"\"Handle this customer support email. Look up their SLA and ticket status first:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "Ticket ID: {LEGIT_EMAIL.ticket_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "\n",
    "result = await support_agent_with_tools.run(prompt)\n",
    "print(\"üìß Response (with tool lookups):\\n\")\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530d0d1",
   "metadata": {},
   "source": [
    "# 5. Human-in-the-Loop Approval\n",
    "\n",
    "Require human confirmation before executing sensitive actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479786af",
   "metadata": {},
   "source": [
    "## Approval-Required Tool\n",
    "\n",
    "Set `approval_mode=\"always_require\"` on sensitive tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9cbdcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ approval_agent created with send_email_reply tool\n"
     ]
    }
   ],
   "source": [
    "from agent_framework import ChatMessage, Content, Role\n",
    "\n",
    "# Tool that requires human approval before sending\n",
    "@tool(approval_mode=\"always_require\", name=\"send_email_reply\", description=\"Send an email reply to the customer. Requires human approval.\")\n",
    "def send_email_reply(\n",
    "    to: Annotated[str, Field(description=\"Recipient email address\")],\n",
    "    subject: Annotated[str, Field(description=\"Email subject\")],\n",
    "    body: Annotated[str, Field(description=\"Email body content\")]\n",
    ") -> str:\n",
    "    \"\"\"Send an email reply to the customer. Requires human approval.\"\"\"\n",
    "    # In production, this would actually send the email\n",
    "    return f\"‚úÖ Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "# Create agent with the approval-required tool\n",
    "approval_agent = chat_client.as_agent(\n",
    "    name=\"ApprovalSupportAgent\",\n",
    "    instructions=\"\"\"You are a customer support agent. After drafting a response, \n",
    "use the send_email_reply tool to send it. This will require human approval.\"\"\",\n",
    "    tools=[lookup_customer_sla, get_incident_status, send_email_reply]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ approval_agent created with send_email_reply tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de90248",
   "metadata": {},
   "source": [
    "## Check for Pending Approvals\n",
    "\n",
    "Approval-required calls return `user_input_requests` instead of executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e6063ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí APPROVAL REQUIRED!\n",
      "  Function: send_email_reply\n",
      "  Arguments: {\"to\":\"sarah.chen@acmecorp.com\",\"subject\":\"Order #12345 - Delivery Issue\",\"body\":\"Hi Sarah,\\n\\nThank you for reaching out regarding your order #12345. I understand the urgency of the situation, especially with your client presentation coming up this Friday.\\n\\nSince your package shows as delivered but has not been received, I will arrange for a replacement to be sent to you as per our policy for Premium tier customers. This will be expedited to ensure you have the items in time for your presentation.\\n\\nOur team will initiate this process right away, and I will keep you updated on the progress. If you have any further questions or need assistance, please feel free to reach out.\\n\\nBest regards,\\nThe Support Team\"}\n"
     ]
    }
   ],
   "source": [
    "# Ask the agent to handle and send a response\n",
    "prompt = f\"\"\"Handle this email and IMMEDIATELY use the send_email_reply tool to send a response. \n",
    "Do not ask for permission - just use the tool directly.\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "\n",
    "result = await approval_agent.run(prompt)\n",
    "\n",
    "# Check if approval is needed\n",
    "if result.user_input_requests:\n",
    "    print(\"üîí APPROVAL REQUIRED!\")\n",
    "    for user_input_needed in result.user_input_requests:\n",
    "        print(f\"  Function: {user_input_needed.function_call.name}\")\n",
    "        print(f\"  Arguments: {user_input_needed.function_call.arguments}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No approval requested - agent didn't call the tool\")\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e80ab2",
   "metadata": {},
   "source": [
    "## Grant Approval\n",
    "\n",
    "Respond with `to_function_approval_response(True/False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa70478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Handling Approval ---\n",
      "\n",
      "‚úÖ Human approved: True\n",
      "\n",
      "üìä Final Result:\n",
      "The email has been successfully sent to Sarah Chen regarding her delivery issue with order #12345. If there's anything else you need, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Handling Approval ---\\n\")\n",
    "\n",
    "# Provide approval and continue the conversation\n",
    "if result.user_input_requests:\n",
    "    user_input_needed = result.user_input_requests[0]\n",
    "    \n",
    "    # Simulate human approval (in production, this would be interactive)\n",
    "    user_approval = True\n",
    "    print(f\"‚úÖ Human approved: {user_approval}\\n\")\n",
    "    \n",
    "    # Create approval response message\n",
    "    approval_message = ChatMessage(\n",
    "        role=Role.USER,\n",
    "        contents=[user_input_needed.to_function_approval_response(user_approval)]\n",
    "    )\n",
    "    \n",
    "    # Continue with approval\n",
    "    final_result = await approval_agent.run([\n",
    "        prompt,\n",
    "        ChatMessage(role=Role.ASSISTANT, contents=[user_input_needed]),\n",
    "        approval_message\n",
    "    ])\n",
    "    print(f\"üìä Final Result:\\n{final_result.text}\")\n",
    "else:\n",
    "    print(\"‚ùå No approval was requested in the previous cell.\")\n",
    "    print(\"   The agent needs to call the send_email_reply tool to trigger approval.\")\n",
    "    print(\"   Re-run the previous cell to try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbefd9d",
   "metadata": {},
   "source": [
    "# 6. Middleware\n",
    "\n",
    "Intercept agent execution for logging, metrics, and observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8829240",
   "metadata": {},
   "source": [
    "## Define Middleware\n",
    "\n",
    "Middleware wraps execution with `context` and `next` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae080f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Middleware defined: logging_agent_middleware, logging_function_middleware\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Awaitable\n",
    "from agent_framework import AgentRunContext, FunctionInvocationContext\n",
    "import time\n",
    "\n",
    "async def logging_agent_middleware(\n",
    "    context: AgentRunContext,\n",
    "    next: Callable[[AgentRunContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    \"\"\"Log agent execution with timing.\"\"\"\n",
    "    print(f\"üöÄ Agent starting... ({len(context.messages)} message(s))\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    await next(context)  # Continue to agent execution\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Agent finished in {elapsed:.2f}s\")\n",
    "\n",
    "async def logging_function_middleware(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    \"\"\"Log function tool calls.\"\"\"\n",
    "    print(f\"  üìû Calling: {context.function.name}({context.arguments})\")\n",
    "    \n",
    "    await next(context)\n",
    "    \n",
    "    print(f\"  üì§ Result: {context.result[:100]}...\" if len(str(context.result)) > 100 else f\"  üì§ Result: {context.result}\")\n",
    "\n",
    "print(\"‚úÖ Middleware defined: logging_agent_middleware, logging_function_middleware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e9a62",
   "metadata": {},
   "source": [
    "## Attach Middleware\n",
    "\n",
    "Pass middleware list when creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f92c1554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Agent starting... (1 message(s))\n",
      "  üìû Calling: lookup_customer_sla(customer_id='CUST-7891')\n",
      "  üì§ Result: Customer CUST-7891: Premium tier, 4 hours response time, Free expedited replacement\n",
      "  üìû Calling: get_incident_status(ticket_id='TKT-2024-001')\n",
      "  üì§ Result: Ticket TKT-2024-001: Status=Open, Priority=High, Assigned to=Support Team, Last update=2024-01-15\n",
      "‚úÖ Agent finished in 3.81s\n",
      "\n",
      "üí¨ Response: Here are the details you requested:\n",
      "\n",
      "### Customer SLA for CUST-7891\n",
      "- **SLA Tier:** Premium\n",
      "- **Response Time:** 4 hours\n",
      "- **Policy:** Free expedited replacement\n",
      "\n",
      "### Ticket Status for TKT-2024-001\n",
      "- **Status:** Open\n",
      "- **Priority:** High\n",
      "- **Assigned to:** Support Team\n",
      "- **Last Update:** January 15, 2024\n",
      "\n",
      "If you need any further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Create agent with middleware for logging\n",
    "middleware_agent = chat_client.as_agent(\n",
    "    name=\"LoggingSupportAgent\",\n",
    "    instructions=\"You are a support agent. Look up customer information when handling requests.\",\n",
    "    tools=[lookup_customer_sla, get_incident_status],\n",
    "    middleware=[logging_agent_middleware, logging_function_middleware]\n",
    ")\n",
    "\n",
    "# Test - you'll see logs for agent and function calls\n",
    "prompt = f\"Check the SLA for customer {LEGIT_EMAIL.customer_id} and ticket status for {LEGIT_EMAIL.ticket_id}\"\n",
    "result = await middleware_agent.run(prompt)\n",
    "print(f\"\\nüí¨ Response: {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffbde4",
   "metadata": {},
   "source": [
    "# 7. Agent Memory\n",
    "\n",
    "Persist context across calls using a `ContextProvider`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ced48",
   "metadata": {},
   "source": [
    "## Preferences Model\n",
    "\n",
    "Define what to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3468ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SupportPreferences model defined\n"
     ]
    }
   ],
   "source": [
    "class SupportPreferences(BaseModel):\n",
    "    \"\"\"User preferences for support interactions.\"\"\"\n",
    "    name: str | None = None\n",
    "    preferred_language: Literal[\"English\", \"Hebrew\", \"Spanish\"] = \"English\"\n",
    "    preferred_tone: Literal[\"formal\", \"friendly\", \"brief\"] = \"formal\"\n",
    "\n",
    "print(\"‚úÖ SupportPreferences model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdd4af",
   "metadata": {},
   "source": [
    "## Implement ContextProvider\n",
    "\n",
    "Two methods: `invoking` (inject context before calls) and `invoked` (extract state after calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082eefb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SupportMemory ContextProvider defined\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import MutableSequence, Sequence\n",
    "from typing import Any\n",
    "\n",
    "from agent_framework import ContextProvider, Context, ChatAgent, ChatOptions\n",
    "\n",
    "\n",
    "class SupportMemory(ContextProvider):\n",
    "    \"\"\"Memory that tracks user preferences for support interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, chat_client, preferences: SupportPreferences | None = None, **kwargs: Any):\n",
    "        \"\"\"Create the memory.\n",
    "        \n",
    "        Args:\n",
    "            chat_client: The chat client to use for extracting structured data\n",
    "            preferences: Optional initial preferences\n",
    "            **kwargs: Additional keyword arguments for deserialization\n",
    "        \"\"\"\n",
    "        self._chat_client = chat_client\n",
    "        if preferences:\n",
    "            self.preferences = preferences\n",
    "        elif kwargs:\n",
    "            self.preferences = SupportPreferences.model_validate(kwargs)\n",
    "        else:\n",
    "            self.preferences = SupportPreferences()\n",
    "    \n",
    "    async def invoked(\n",
    "        self,\n",
    "        request_messages: ChatMessage | Sequence[ChatMessage],\n",
    "        response_messages: ChatMessage | Sequence[ChatMessage] | None = None,\n",
    "        invoke_exception: Exception | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Extract preferences from user messages after each call.\"\"\"\n",
    "        # Ensure request_messages is a list\n",
    "        messages_list = [request_messages] if isinstance(request_messages, ChatMessage) else list(request_messages)\n",
    "        \n",
    "        # Check if we have user messages\n",
    "        user_messages = [msg for msg in messages_list if msg.role.value == \"user\"]\n",
    "        \n",
    "        if user_messages:\n",
    "            try:\n",
    "                # Use the chat client to extract structured information\n",
    "                # NOTE: Use `options=` not `chat_options=`\n",
    "                result = await self._chat_client.get_response(\n",
    "                    messages=messages_list,\n",
    "                    options=ChatOptions(\n",
    "                        instructions=(\n",
    "                            \"Extract the user's name, preferred tone (formal/friendly/brief), \"\n",
    "                            \"and preferred language (English/Hebrew/Spanish) from the messages if present. \"\n",
    "                            \"If not present, return None for that field.\"\n",
    "                        ),\n",
    "                        response_format=SupportPreferences,\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                # result.value should now be a SupportPreferences instance\n",
    "                extracted = result.value\n",
    "                \n",
    "                # Update preferences with extracted data\n",
    "                if extracted and isinstance(extracted, SupportPreferences):\n",
    "                    if self.preferences.name is None and extracted.name:\n",
    "                        self.preferences.name = extracted.name\n",
    "                        print(f\"   üß† Memory updated: name = {extracted.name}\")\n",
    "                    \n",
    "                    if extracted.preferred_tone != \"formal\":  # formal is default\n",
    "                        self.preferences.preferred_tone = extracted.preferred_tone\n",
    "                        print(f\"   üß† Memory updated: tone = {extracted.preferred_tone}\")\n",
    "                    \n",
    "                    if extracted.preferred_language != \"English\":  # English is default\n",
    "                        self.preferences.preferred_language = extracted.preferred_language\n",
    "                        print(f\"   üß† Memory updated: language = {extracted.preferred_language}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to extract preferences: {e}\")\n",
    "    \n",
    "    async def invoking(self, messages: ChatMessage | MutableSequence[ChatMessage], **kwargs: Any) -> Context:\n",
    "        \"\"\"Provide preference context before each agent call.\"\"\"\n",
    "        instructions: list[str] = []\n",
    "        \n",
    "        if self.preferences.name:\n",
    "            instructions.append(f\"The user's name is {self.preferences.name}. Address them by name.\")\n",
    "        \n",
    "        instructions.append(f\"Respond in {self.preferences.preferred_language}.\")\n",
    "        instructions.append(f\"Use a {self.preferences.preferred_tone} tone.\")\n",
    "        \n",
    "        return Context(instructions=\" \".join(instructions))\n",
    "    \n",
    "    def serialize(self) -> str:\n",
    "        \"\"\"Serialize for persistence.\"\"\"\n",
    "        return self.preferences.model_dump_json()\n",
    "\n",
    "print(\"‚úÖ SupportMemory ContextProvider defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d6fd8",
   "metadata": {},
   "source": [
    "## Test Memory\n",
    "\n",
    "The agent automatically extracts and applies preferences across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417d9431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: User introduction\n",
      "--------------------------------------------------\n",
      "   üß† Memory updated: name = David\n",
      "   üß† Memory updated: tone = friendly\n",
      "Agent: Hello, David! How may I assist you today?\n",
      "\n",
      "Turn 2: Setting preference\n",
      "--------------------------------------------------\n",
      "   üß† Memory updated: tone = brief\n",
      "Agent: Got it, David! I‚Äôll keep it short and chill. What can I help you with today?\n",
      "\n",
      "Turn 3: Question with preferences applied\n",
      "--------------------------------------------------\n",
      "   üß† Memory updated: tone = brief\n",
      "Agent: Hi David! Our return policy typically allows for returns within 30 days of purchase, as long as items are in original condition. For full details, it's best to check our website or contact customer service directly. Let me know if you need more help!\n",
      "\n",
      "üß† Memory State (tracked by ContextProvider):\n",
      "   Name: David\n",
      "   Language: English\n",
      "   Tone: brief\n"
     ]
    }
   ],
   "source": [
    "# Create the memory provider using the existing chat_client\n",
    "support_memory = SupportMemory(chat_client)\n",
    "\n",
    "# Create the agent with memory\n",
    "memory_agent = ChatAgent(\n",
    "    name=\"MemorySupportAgent\",\n",
    "    instructions=\"You are a friendly support agent. Adapt your responses based on user preferences.\",\n",
    "    chat_client=chat_client,\n",
    "    context_provider=support_memory,\n",
    ")\n",
    "\n",
    "# Turn 1: User introduces themselves\n",
    "print(\"Turn 1: User introduction\")\n",
    "print(\"-\" * 50)\n",
    "result1 = await memory_agent.run(\"Hi, my name is David\")\n",
    "print(f\"Agent: {result1.text}\\n\")\n",
    "\n",
    "# Turn 2: User sets preference\n",
    "print(\"Turn 2: Setting preference\")\n",
    "print(\"-\" * 50)\n",
    "result2 = await memory_agent.run(\"Please keep responses brief and casual\")\n",
    "print(f\"Agent: {result2.text}\\n\")\n",
    "\n",
    "# Turn 3: Ask a question - memory should apply name and brief tone\n",
    "print(\"Turn 3: Question with preferences applied\")\n",
    "print(\"-\" * 50)\n",
    "result3 = await memory_agent.run(\"What's your return policy?\")\n",
    "print(f\"Agent: {result3.text}\\n\")\n",
    "\n",
    "# Check memory state - access the original support_memory object directly\n",
    "print(\"üß† Memory State (tracked by ContextProvider):\")\n",
    "print(f\"   Name: {support_memory.preferences.name}\")\n",
    "print(f\"   Language: {support_memory.preferences.preferred_language}\")\n",
    "print(f\"   Tone: {support_memory.preferences.preferred_tone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8a13c",
   "metadata": {},
   "source": [
    "## Workflows Overview\n",
    "\n",
    "**Workflows** enable orchestrating multiple agents, humans, and external systems in a predefined execution graph.\n",
    "\n",
    "### Agent vs. Workflow\n",
    "\n",
    "| AI Agent | Workflow |\n",
    "|----------|----------|\n",
    "| Single reasoning loop | Orchestrates multiple components |\n",
    "| Dynamic tool selection | Predefined execution paths |\n",
    "| Best for: focused tasks | Best for: multi-step processes |\n",
    "\n",
    "### When to Use Workflows\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| **Sequential** | Steps must run in order (classify ‚Üí draft ‚Üí review) |\n",
    "| **Branching** | Different paths based on conditions (spam vs. legitimate) |\n",
    "| **Parallel (Fan-out/Fan-in)** | Independent tasks that can run concurrently |\n",
    "| **Group Chat** | Iterative refinement with multiple reviewers |\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Executor** | Unit of work ‚Äî agent or custom logic |\n",
    "| **Edge** | Connection between executors with optional conditions |\n",
    "| **WorkflowBuilder** | Constructs the execution graph |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429fcad",
   "metadata": {},
   "source": [
    "# 8. Sequential Workflows\n",
    "\n",
    "![Sequential Workflow](images/sequential-workflow.png)\n",
    "\n",
    "Chain multiple agents/executors in sequence: Classify ‚Üí Draft ‚Üí Review.\n",
    "\n",
    "**When to Use:**\n",
    "- Tasks with clear, ordered steps (e.g., parse ‚Üí validate ‚Üí transform)\n",
    "- When each step's output is the next step's input\n",
    "- Processing pipelines where order matters\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Steps can run independently (use Concurrent instead)\n",
    "- Dynamic routing needed (use Branching instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6274db5",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Executor** | Unit of work (`@executor` or class with `@handler`) |\n",
    "| **WorkflowBuilder** | Connects executors with `add_edge()` |\n",
    "| `ctx.send_message()` | Pass data to next executor |\n",
    "| `ctx.yield_output()` | Return final result |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80541f",
   "metadata": {},
   "source": [
    "## Define Executors\n",
    "\n",
    "Create agent executors for classification, writing, and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "276a9eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow agents defined: classifier, writer, reviewer\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Never\n",
    "from agent_framework import (\n",
    "    WorkflowBuilder, WorkflowContext, WorkflowOutputEvent,\n",
    "    Executor, executor, handler, AgentExecutor, AgentExecutorRequest, AgentExecutorResponse\n",
    ")\n",
    "\n",
    "# === CLASSIFIER AGENT ===\n",
    "classifier_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Classifier\",\n",
    "        instructions=\"\"\"Classify incoming emails. Return JSON with:\n",
    "- category: \"spam\", \"not_spam\", or \"uncertain\"\n",
    "- confidence: float 0-1\n",
    "- reason: brief explanation\"\"\",\n",
    "        response_format=ClassificationResult,\n",
    "    ),\n",
    "    id=\"classifier\",\n",
    ")\n",
    "\n",
    "# === DRAFT WRITER AGENT ===\n",
    "writer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"DraftWriter\",\n",
    "        instructions=\"\"\"Draft professional support responses. Return JSON with:\n",
    "- subject: reply subject line\n",
    "- body: reply body\n",
    "- tone: \"formal\", \"friendly\", or \"apologetic\"\n",
    "- needs_review: true if sensitive or complex\"\"\",\n",
    "        response_format=DraftResponse,\n",
    "    ),\n",
    "    id=\"writer\",\n",
    ")\n",
    "\n",
    "# === REVIEWER AGENT ===\n",
    "reviewer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Reviewer\",\n",
    "        instructions=\"\"\"Review draft responses for quality. Check:\n",
    "- Professionalism and tone\n",
    "- Accuracy of information\n",
    "- Completeness\n",
    "Return approval decision with notes.\"\"\",\n",
    "    ),\n",
    "    id=\"reviewer\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Workflow agents defined: classifier, writer, reviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f78d8",
   "metadata": {},
   "source": [
    "## Build & Run\n",
    "\n",
    "Connect executors with `add_edge()` and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94b511c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Processing email through workflow: Classify ‚Üí Draft ‚Üí Review\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ [classifier]:\n",
      "   ```json\n",
      "{\n",
      "  \"category\": \"not_spam\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"reason\": \"The email is a legitimate support request regarding a specific order issue from a known domain, describing an urgent situation that requires assistance.\"\n",
      "}\n",
      "```...\n",
      "\n",
      "‚úÖ [writer]:\n",
      "   {\n",
      "  \"subject\": \"Re: Order #12345 - Delivery Issue\",\n",
      "  \"body\": \"Dear Sarah,\\n\\nThank you for contacting us regarding the delivery issue with your order #12345. I sincerely apologize for the inconvenience this has caused, especially with your client presentation approaching.\\n\\nWe will investigate the...\n",
      "\n",
      "‚úÖ [reviewer]:\n",
      "   **Approval Decision: Approved**\n",
      "\n",
      "**Notes:**\n",
      "\n",
      "- **Professionalism and Tone:** The response maintains a formal and empathetic tone, which is appropriate given the urgency of the customer‚Äôs situation. The acknowledgement of the inconvenience shows respect for the customer‚Äôs experience.\n",
      "\n",
      "- **Accuracy of...\n"
     ]
    }
   ],
   "source": [
    "# Build sequential workflow\n",
    "sequential_support_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(classifier_agent)\n",
    "    .add_edge(classifier_agent, writer_agent)\n",
    "    .add_edge(writer_agent, reviewer_agent)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run with legitimate email\n",
    "async def run_sequential_workflow():\n",
    "    email_prompt = f\"\"\"Process this support email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"üìß Processing email through workflow: Classify ‚Üí Draft ‚Üí Review\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    request = AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=email_prompt)],\n",
    "        should_respond=True\n",
    "    )\n",
    "    \n",
    "    from agent_framework._workflows._events import ExecutorCompletedEvent\n",
    "    \n",
    "    async for event in sequential_support_workflow.run_stream(request):\n",
    "        if isinstance(event, ExecutorCompletedEvent) and event.data:\n",
    "            data = event.data[0] if isinstance(event.data, list) else event.data\n",
    "            if hasattr(data, 'agent_response'):\n",
    "                print(f\"\\n‚úÖ [{event.executor_id}]:\")\n",
    "                print(f\"   {data.agent_response.text[:300]}...\")\n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            print(f\"\\nüéØ FINAL OUTPUT:\")\n",
    "            if isinstance(event.data, list) and event.data:\n",
    "                final = event.data[0]\n",
    "                if hasattr(final, 'agent_response'):\n",
    "                    print(final.agent_response.text)\n",
    "\n",
    "await run_sequential_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37a59d",
   "metadata": {},
   "source": [
    "# 9. Branching Logic\n",
    "\n",
    "Route execution based on conditions: Spam ‚Üí Block, NotSpam ‚Üí Draft, Uncertain ‚Üí Review.\n",
    "\n",
    "**When to Use:**\n",
    "- Different paths based on classification or conditions\n",
    "- Error handling with fallback routes\n",
    "- Multi-way routing (switch-case patterns)\n",
    "\n",
    "**When NOT to Use:**\n",
    "- All items follow the same path (use Sequential)\n",
    "- Need parallel execution of branches (use Fan-Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec9cf6",
   "metadata": {},
   "source": [
    "## Routing Patterns\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| **Conditional Edge** | Binary if/else |\n",
    "| **Switch-Case** | Multi-way routing |\n",
    "| **Multi-Selection** | Dynamic fan-out |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60db70",
   "metadata": {},
   "source": [
    "## Define Branch Handlers\n",
    "\n",
    "Create handlers for each classification outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38082206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Branching executors defined\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from uuid import uuid4\n",
    "from agent_framework import Case, Default\n",
    "\n",
    "# Internal payload for routing\n",
    "@dataclass\n",
    "class ClassifiedEmail:\n",
    "    email_id: str\n",
    "    category: str  # spam, not_spam, uncertain\n",
    "    confidence: float\n",
    "    reason: str\n",
    "    original_content: str\n",
    "\n",
    "# Shared state keys\n",
    "EMAIL_KEY = \"current_email\"\n",
    "\n",
    "# Helper to extract JSON from markdown code blocks\n",
    "def extract_json(text: str) -> str:\n",
    "    \"\"\"Extract JSON from text, stripping markdown code blocks if present.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Transform classification result to routable payload\n",
    "@executor(id=\"extract_classification\")\n",
    "async def extract_classification(response: Any, ctx: WorkflowContext[ClassifiedEmail]) -> None:\n",
    "    \"\"\"Extract classification from agent response for routing.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        response = response[0]\n",
    "    \n",
    "    # Extract JSON (handles markdown code blocks)\n",
    "    json_text = extract_json(response.agent_response.text)\n",
    "    classification = ClassificationResult.model_validate_json(json_text)\n",
    "    \n",
    "    # Get original email from shared state\n",
    "    original_content = await ctx.get_shared_state(EMAIL_KEY) or \"Unknown\"\n",
    "    \n",
    "    payload = ClassifiedEmail(\n",
    "        email_id=str(uuid4()),\n",
    "        category=classification.category,\n",
    "        confidence=classification.confidence,\n",
    "        reason=classification.reason,\n",
    "        original_content=original_content\n",
    "    )\n",
    "    await ctx.send_message(payload)\n",
    "\n",
    "# Route conditions\n",
    "def is_spam(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"spam\"\n",
    "\n",
    "def is_not_spam(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"not_spam\"\n",
    "\n",
    "def is_uncertain(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"uncertain\"\n",
    "\n",
    "# Terminal handlers\n",
    "@executor(id=\"handle_spam\")\n",
    "async def handle_spam_terminal(email: ClassifiedEmail, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Handle spam: block and log.\"\"\"\n",
    "    await ctx.yield_output(f\"üö´ SPAM BLOCKED: {email.reason} (confidence: {email.confidence:.0%})\")\n",
    "\n",
    "@executor(id=\"handle_not_spam\")\n",
    "async def handle_not_spam_continue(email: ClassifiedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Handle not_spam: forward to writer.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to: {email.original_content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "@executor(id=\"finalize_draft\")\n",
    "async def finalize_draft(response: Any, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Output the final draft.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        response = response[0]\n",
    "    # Extract JSON (handles markdown code blocks)\n",
    "    json_text = extract_json(response.agent_response.text)\n",
    "    draft = DraftResponse.model_validate_json(json_text)\n",
    "    await ctx.yield_output(f\"‚úâÔ∏è DRAFT READY:\\nSubject: {draft.subject}\\n\\n{draft.body}\")\n",
    "\n",
    "@executor(id=\"handle_uncertain\")\n",
    "async def handle_uncertain_terminal(email: ClassifiedEmail, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Handle uncertain: flag for human review.\"\"\"\n",
    "    await ctx.yield_output(f\"‚ö†Ô∏è NEEDS HUMAN REVIEW: {email.reason} (confidence: {email.confidence:.0%})\\n\\nOriginal: {email.original_content[:200]}...\")\n",
    "\n",
    "print(\"‚úÖ Branching executors defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f06000",
   "metadata": {},
   "source": [
    "## Build Switch-Case Workflow\n",
    "\n",
    "Route based on classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840d3b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Branching workflow built\n"
     ]
    }
   ],
   "source": [
    "# Store email and start classification\n",
    "@executor(id=\"start_classification\")\n",
    "async def start_classification(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Store email and send for classification.\"\"\"\n",
    "    await ctx.set_shared_state(EMAIL_KEY, email_text)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Classify this email:\\n\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Build branching workflow\n",
    "branching_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(start_classification)\n",
    "    .add_edge(start_classification, classifier_agent)\n",
    "    .add_edge(classifier_agent, extract_classification)\n",
    "    # Switch-case routing\n",
    "    .add_switch_case_edge_group(\n",
    "        extract_classification,\n",
    "        [\n",
    "            Case(condition=is_spam, target=handle_spam_terminal),\n",
    "            Case(condition=is_not_spam, target=handle_not_spam_continue),\n",
    "            Default(target=handle_uncertain_terminal),  # Catches uncertain + unexpected\n",
    "        ],\n",
    "    )\n",
    "    # Continue not_spam path to draft\n",
    "    .add_edge(handle_not_spam_continue, writer_agent)\n",
    "    .add_edge(writer_agent, finalize_draft)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Branching workflow built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6f304",
   "metadata": {},
   "source": [
    "## Test Branching\n",
    "\n",
    "Run all three email types through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59110839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìß Testing LEGITIMATE email...\n",
      "--------------------------------------------------\n",
      "‚úâÔ∏è DRAFT READY:\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "Thank you for bringing this issue to our attention. I sincerely apologize for the inconvenience you've faced with your order #12345 not being delivered, especially given the urgency of your client presentation.\n",
      "\n",
      "We understand how important it is to have your items on time. I will immediately investigate the delivery status with our shipping partner and will do everything possible to locate your package. If we are unable to find it shortly, we will arrange for a replacement to be sent right away.\n",
      "\n",
      "Please allow us a little time to get back to you with an update.\n",
      "\n",
      "Thank you for your understanding and patience.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "Customer Support Team\n",
      "\n",
      "üìß Testing SPAM email...\n",
      "--------------------------------------------------\n",
      "üö´ SPAM BLOCKED: The email is from a suspicious domain known for scams, makes unrealistic claims about winning a lottery, requests sensitive personal information, and uses urgency to prompt immediate action. (confidence: 99%)\n",
      "\n",
      "üìß Testing AMBIGUOUS email...\n",
      "--------------------------------------------------\n",
      "‚ö†Ô∏è NEEDS HUMAN REVIEW: The email is from an unknown domain and presents a vague business opportunity, making it difficult to ascertain its legitimacy. There is no clear indication of spam, but the lack of recognizable context raises suspicion. (confidence: 60%)\n",
      "\n",
      "Original: From: j.smith@unknown-domain.net\n",
      "Subject: Partnership Opportunity\n",
      "\n",
      "Hello,\n",
      "\n",
      "I found your company online and I'm interested in discussing a potential \n",
      "business partnership. We have a new product line th...\n"
     ]
    }
   ],
   "source": [
    "# Test all three paths\n",
    "async def test_branching():\n",
    "    test_cases = [\n",
    "        (\"LEGITIMATE\", LEGIT_EMAIL),\n",
    "        (\"SPAM\", SPAM_EMAIL),\n",
    "        (\"AMBIGUOUS\", AMBIGUOUS_EMAIL),\n",
    "    ]\n",
    "    \n",
    "    for label, email in test_cases:\n",
    "        print(f\"\\nüìß Testing {label} email...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        email_text = f\"From: {email.sender}\\nSubject: {email.subject}\\n\\n{email.body}\"\n",
    "        \n",
    "        async for event in branching_workflow.run_stream(email_text):\n",
    "            if isinstance(event, WorkflowOutputEvent):\n",
    "                print(event.data)\n",
    "\n",
    "await test_branching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecb767",
   "metadata": {},
   "source": [
    "# 10. Fan-Out / Fan-In\n",
    "\n",
    "![Concurrent Workflow](images/concurrent-workflow.png)\n",
    "\n",
    "Process multiple paths in parallel and aggregate results.\n",
    "\n",
    "**When to Use:**\n",
    "- Independent tasks that can run concurrently\n",
    "- Aggregating results from multiple sources\n",
    "- Performance optimization through parallelization\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Tasks have dependencies on each other\n",
    "- Order of execution matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863e4e6",
   "metadata": {},
   "source": [
    "## Define Parallel Paths\n",
    "\n",
    "For long emails: respond AND summarize concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d8e251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parallel processing executors defined\n"
     ]
    }
   ],
   "source": [
    "# Summary model\n",
    "class EmailSummary(BaseModel):\n",
    "    \"\"\"Concise email summary.\"\"\"\n",
    "    key_points: list[str] = Field(description=\"Main points from the email\")\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Urgency level\")\n",
    "    action_required: str = Field(description=\"Primary action needed\")\n",
    "\n",
    "# Summarizer agent\n",
    "summarizer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Summarizer\",\n",
    "        instructions=\"\"\"Summarize emails concisely. Return JSON with:\n",
    "- key_points: list of main points\n",
    "- urgency: low/medium/high\n",
    "- action_required: primary action needed\"\"\",\n",
    "        response_format=EmailSummary,\n",
    "    ),\n",
    "    id=\"summarizer\",\n",
    ")\n",
    "\n",
    "# Threshold for \"long\" emails\n",
    "LONG_EMAIL_THRESHOLD = 200  # characters\n",
    "\n",
    "@dataclass\n",
    "class EnrichedEmail:\n",
    "    \"\"\"Email with metadata for routing.\"\"\"\n",
    "    email_id: str\n",
    "    content: str\n",
    "    is_long: bool\n",
    "    category: str\n",
    "\n",
    "# Selection function for multi-selection routing\n",
    "def select_parallel_paths(email: EnrichedEmail, target_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Select paths based on email length.\"\"\"\n",
    "    # target_ids order: [respond_path, summarize_path]\n",
    "    respond_id, summarize_id = target_ids\n",
    "    \n",
    "    if email.is_long:\n",
    "        return [respond_id, summarize_id]  # Both paths in parallel\n",
    "    else:\n",
    "        return [respond_id]  # Only respond for short emails\n",
    "\n",
    "# Executors for parallel paths\n",
    "@executor(id=\"prepare_parallel\")\n",
    "async def prepare_parallel(classified: ClassifiedEmail, ctx: WorkflowContext[EnrichedEmail]) -> None:\n",
    "    \"\"\"Prepare email for parallel processing.\"\"\"\n",
    "    enriched = EnrichedEmail(\n",
    "        email_id=classified.email_id,\n",
    "        content=classified.original_content,\n",
    "        is_long=len(classified.original_content) > LONG_EMAIL_THRESHOLD,\n",
    "        category=classified.category\n",
    "    )\n",
    "    await ctx.send_message(enriched)\n",
    "\n",
    "@executor(id=\"respond_path\")\n",
    "async def respond_path(email: EnrichedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Send to writer for response.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to:\\n{email.content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "@executor(id=\"summarize_path\")\n",
    "async def summarize_path(email: EnrichedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Send to summarizer.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Summarize this email:\\n{email.content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Aggregator to combine parallel results\n",
    "class ParallelAggregator(Executor):\n",
    "    def __init__(self):\n",
    "        super().__init__(id=\"parallel_aggregator\")\n",
    "    \n",
    "    @handler\n",
    "    async def aggregate(self, results: list[Any], ctx: WorkflowContext[Never, str]) -> None:\n",
    "        \"\"\"Combine response and summary.\"\"\"\n",
    "        output_parts = []\n",
    "        \n",
    "        for result in results:\n",
    "            if isinstance(result, AgentExecutorResponse):\n",
    "                try:\n",
    "                    draft = DraftResponse.model_validate_json(result.agent_response.text)\n",
    "                    output_parts.append(f\"üìß DRAFT RESPONSE:\\nSubject: {draft.subject}\\n{draft.body}\")\n",
    "                except:\n",
    "                    try:\n",
    "                        summary = EmailSummary.model_validate_json(result.agent_response.text)\n",
    "                        points = \"\\n\".join(f\"  ‚Ä¢ {p}\" for p in summary.key_points)\n",
    "                        output_parts.append(f\"üìã SUMMARY:\\n{points}\\nUrgency: {summary.urgency}\\nAction: {summary.action_required}\")\n",
    "                    except:\n",
    "                        output_parts.append(f\"Result: {result.agent_response.text[:200]}...\")\n",
    "        \n",
    "        await ctx.yield_output(\"\\n\\n\" + \"=\"*40 + \"\\n\\n\".join(output_parts))\n",
    "\n",
    "aggregator = ParallelAggregator()\n",
    "\n",
    "print(\"‚úÖ Parallel processing executors defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ef984",
   "metadata": {},
   "source": [
    "## Build Fan-Out/Fan-In Workflow\n",
    "\n",
    "Short emails ‚Üí respond only. Long emails ‚Üí respond + summarize in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e887adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fan-out/fan-in workflow built\n"
     ]
    }
   ],
   "source": [
    "from agent_framework import WorkflowBuilder\n",
    "from agent_framework._workflows._events import ExecutorCompletedEvent\n",
    "from datetime import datetime\n",
    "\n",
    "# Constants\n",
    "LONG_EMAIL_THRESHOLD = 200  # Characters\n",
    "\n",
    "# Start executor - entry point stores email and passes it forward\n",
    "@executor(id=\"fanout_start\")\n",
    "async def fanout_start(email_text: str, ctx: WorkflowContext[str]) -> None:\n",
    "    \"\"\"Entry point: store email length, forward email text.\"\"\"\n",
    "    # Store email length in shared state for selection\n",
    "    await ctx.set_shared_state(\"email_length\", len(email_text))\n",
    "    # Store workflow start time\n",
    "    await ctx.set_shared_state(\"workflow_start_time\", time.time())\n",
    "    await ctx.send_message(email_text)\n",
    "\n",
    "# Selection function that uses shared state\n",
    "def fanout_select_paths(email_text: str, target_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Select paths based on email length (stored in text).\"\"\"\n",
    "    # The email_text is still the raw string at this point\n",
    "    if len(email_text) > LONG_EMAIL_THRESHOLD:\n",
    "        return target_ids  # Both paths for long emails\n",
    "    return [target_ids[0]]  # Only response path for short emails\n",
    "\n",
    "# Response path preparer with timing\n",
    "@executor(id=\"fanout_respond_prep\")\n",
    "async def fanout_respond_prep(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Prepare email for writer agent.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    start_time = time.time()\n",
    "    elapsed = start_time - workflow_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed:.2f}s] üìù RESPONSE PATH started\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"response_start_time\", start_time)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to:\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Summary path preparer with timing\n",
    "@executor(id=\"fanout_summarize_prep\")\n",
    "async def fanout_summarize_prep(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Prepare email for summarizer agent.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    start_time = time.time()\n",
    "    elapsed = start_time - workflow_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed:.2f}s] üìã SUMMARY PATH started\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"summary_start_time\", start_time)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Summarize this email:\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Capture completion time immediately after writer finishes\n",
    "@executor(id=\"capture_writer_completion\")\n",
    "async def capture_writer_completion(result: Any, ctx: WorkflowContext[Any]) -> None:\n",
    "    \"\"\"Capture writer completion time.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    response_start = await ctx.get_shared_state(\"response_start_time\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_from_start = end_time - workflow_start\n",
    "    duration = end_time - response_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed_from_start:.2f}s] ‚úÖ RESPONSE PATH completed ({duration:.2f}s)\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"response_end_time\", end_time)\n",
    "    await ctx.send_message(result)\n",
    "\n",
    "# Capture completion time immediately after summarizer finishes\n",
    "@executor(id=\"capture_summarizer_completion\")\n",
    "async def capture_summarizer_completion(result: Any, ctx: WorkflowContext[Any]) -> None:\n",
    "    \"\"\"Capture summarizer completion time.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    summary_start = await ctx.get_shared_state(\"summary_start_time\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_from_start = end_time - workflow_start\n",
    "    duration = end_time - summary_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed_from_start:.2f}s] ‚úÖ SUMMARY PATH completed ({duration:.2f}s)\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"summary_end_time\", end_time)\n",
    "    await ctx.send_message(result)\n",
    "\n",
    "# Aggregator - combines results from parallel paths with timing\n",
    "@executor(id=\"fanout_aggregator\")\n",
    "async def fanout_aggregator(results: list[Any], ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Combine response and summary results with timing information.\"\"\"\n",
    "    response_start = await ctx.get_shared_state(\"response_start_time\")\n",
    "    summary_start = await ctx.get_shared_state(\"summary_start_time\")\n",
    "    response_end = await ctx.get_shared_state(\"response_end_time\")\n",
    "    summary_end = await ctx.get_shared_state(\"summary_end_time\")\n",
    "    \n",
    "    output_parts = []\n",
    "    response_time = None\n",
    "    summary_time = None\n",
    "    \n",
    "    # Calculate durations from stored times\n",
    "    if response_start and response_end:\n",
    "        response_time = response_end - response_start\n",
    "    if summary_start and summary_end:\n",
    "        summary_time = summary_end - summary_start\n",
    "    \n",
    "    for result in results:\n",
    "        if isinstance(result, AgentExecutorResponse):\n",
    "            try:\n",
    "                draft = DraftResponse.model_validate_json(extract_json(result.agent_response.text))\n",
    "                output_parts.append(\n",
    "                    f\"üì¨ RESPONSE (completed in {response_time:.2f}s):\\n\"\n",
    "                    f\"Subject: {draft.subject}\\n{draft.body}\"\n",
    "                )\n",
    "            except:\n",
    "                try:\n",
    "                    summary = EmailSummary.model_validate_json(extract_json(result.agent_response.text))\n",
    "                    points = \"\\n\".join(f\"  ‚Ä¢ {p}\" for p in summary.key_points)\n",
    "                    output_parts.append(\n",
    "                        f\"üìã SUMMARY (completed in {summary_time:.2f}s):\\n\"\n",
    "                        f\"{points}\\n\"\n",
    "                        f\"Urgency: {summary.urgency}\\n\"\n",
    "                        f\"Action: {summary.action_required}\"\n",
    "                    )\n",
    "                except:\n",
    "                    output_parts.append(f\"Result: {result.agent_response.text[:200]}...\")\n",
    "    \n",
    "    # Calculate overlap to show parallelization\n",
    "    if response_time and summary_time:\n",
    "        total_sequential = response_time + summary_time\n",
    "        total_parallel = max(response_time, summary_time)\n",
    "        time_saved = total_sequential - total_parallel\n",
    "        output_parts.append(\n",
    "            f\"\\n‚ö° PARALLEL EXECUTION BENEFIT:\\n\"\n",
    "            f\"   Sequential time: {total_sequential:.2f}s\\n\"\n",
    "            f\"   Parallel time: {total_parallel:.2f}s\\n\"\n",
    "            f\"   Time saved: {time_saved:.2f}s ({time_saved/total_sequential*100:.1f}%)\"\n",
    "        )\n",
    "    \n",
    "    await ctx.yield_output(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(output_parts))\n",
    "\n",
    "# Build the fan-out workflow\n",
    "# Pattern: start -> [fanout to preparers] -> [agents] -> [capture timing] -> aggregator\n",
    "fanout_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(fanout_start)\n",
    "    # Fan-out from start directly to path preparers based on email length\n",
    "    .add_multi_selection_edge_group(\n",
    "        fanout_start,\n",
    "        targets=[fanout_respond_prep, fanout_summarize_prep],\n",
    "        selection_func=fanout_select_paths,\n",
    "    )\n",
    "    # Each preparer sends to its agent\n",
    "    .add_edge(fanout_respond_prep, writer_agent)\n",
    "    .add_edge(fanout_summarize_prep, summarizer_agent)\n",
    "    # Capture completion times immediately after each agent\n",
    "    .add_edge(writer_agent, capture_writer_completion)\n",
    "    .add_edge(summarizer_agent, capture_summarizer_completion)\n",
    "    # Fan-in: collect all results\n",
    "    .add_fan_in_edges([capture_writer_completion, capture_summarizer_completion], fanout_aggregator)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fan-out/fan-in workflow built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b962d58",
   "metadata": {},
   "source": [
    "## Test Parallel Execution\n",
    "\n",
    "Long emails trigger both response and summary paths concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7492286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Testing LONG email (468 chars > 200 threshold)\n",
      "Expected: Response AND Summary in parallel\n",
      "\n",
      "------------------------------------------------------------\n",
      "   ‚è±Ô∏è  [+0.00s] üìù RESPONSE PATH started\n",
      "   ‚è±Ô∏è  [+0.00s] üìã SUMMARY PATH started\n",
      "   ‚è±Ô∏è  [+3.86s] ‚úÖ SUMMARY PATH completed (3.86s)\n",
      "   ‚è±Ô∏è  [+3.88s] ‚úÖ RESPONSE PATH completed (3.88s)\n",
      "\n",
      "\n",
      "==================================================üì¨ RESPONSE (completed in 3.88s):\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "Dear Sarah,\n",
      "\n",
      "Thank you for reaching out and sharing your concern regarding the delivery of your order #12345. I apologize for any inconvenience this situation may have caused, especially with your upcoming client presentation.\n",
      "\n",
      "I completely understand the urgency of resolving this issue. I will initiate an investigation to determine the whereabouts of your package and liaise with our shipping team to get more information. If we are unable to locate the package promptly, we will arrange for a replacement to ensure you receive your items in time for your presentation.\n",
      "\n",
      "I appreciate your patience while we work on this, and I will keep you updated on our progress.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "Customer Support Team\n",
      "\n",
      "üìã SUMMARY (completed in 3.86s):\n",
      "  ‚Ä¢ Order #12345 shows delivered, but not received\n",
      "  ‚Ä¢ Checked with neighbors and concierge, no one has seen it\n",
      "  ‚Ä¢ Items are needed for a client presentation on Friday\n",
      "  ‚Ä¢ Request for help to locate the package or arrange a replacement\n",
      "Urgency: high\n",
      "Action: Locate the package or arrange a replacement\n",
      "\n",
      "\n",
      "‚ö° PARALLEL EXECUTION BENEFIT:\n",
      "   Sequential time: 7.73s\n",
      "   Parallel time: 3.88s\n",
      "   Time saved: 3.86s (49.9%)\n"
     ]
    }
   ],
   "source": [
    "# Test with long legitimate email\n",
    "async def test_fanout():\n",
    "    email_text = f\"From: {LEGIT_EMAIL.sender}\\nSubject: {LEGIT_EMAIL.subject}\\n\\n{LEGIT_EMAIL.body}\"\n",
    "    \n",
    "    print(f\"üìß Testing LONG email ({len(email_text)} chars > {LONG_EMAIL_THRESHOLD} threshold)\")\n",
    "    print(\"Expected: Response AND Summary in parallel\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    async for event in fanout_workflow.run_stream(email_text):\n",
    "        if isinstance(event, WorkflowOutputEvent):\n",
    "            print(event.data)\n",
    "\n",
    "await test_fanout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a7439",
   "metadata": {},
   "source": [
    "# 11. Group Chat Orchestration\n",
    "\n",
    "![Group Chat Pattern](images/group-chat.png)\n",
    "\n",
    "Multiple agents collaborate in a shared conversation, coordinated by an orchestrator.\n",
    "\n",
    "**When to Use:**\n",
    "- Iterative refinement with multiple review rounds\n",
    "- Collaborative problem-solving with shared context\n",
    "- Multi-perspective analysis (e.g., writer-reviewer workflows)\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Agents should work independently (use Concurrent)\n",
    "- Complex dynamic planning needed (use Magentic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38398cec",
   "metadata": {},
   "source": [
    "## Key Differences\n",
    "\n",
    "| Pattern | Coordination | Use Case |\n",
    "|---------|--------------|----------|\n",
    "| **Concurrent** | No coordination | Independent parallel tasks |\n",
    "| **Group Chat** | Orchestrator selects speakers | Iterative refinement, shared context |\n",
    "| **Magentic** | Manager with dynamic planning | Complex open-ended tasks |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105e4d2",
   "metadata": {},
   "source": [
    "## Define Specialists\n",
    "\n",
    "Create agents with distinct review roles. All agents will see the shared conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74b41c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Three specialist reviewers defined:\n",
      "   1. SecurityReviewer - identifies security issues\n",
      "   2. AccuracyReviewer - checks facts and promises\n",
      "   3. ToneReviewer - applies all feedback and produces FINAL email\n"
     ]
    }
   ],
   "source": [
    "from agent_framework import GroupChatBuilder, GroupChatState, ConcurrentBuilder, MagenticBuilder\n",
    "\n",
    "# Three specialized reviewers - order matters! Last one produces final output.\n",
    "\n",
    "# 1st: Security reviewer - identifies security/compliance issues\n",
    "security_reviewer = ChatAgent(\n",
    "    name=\"SecurityReviewer\",\n",
    "    description=\"Security and compliance specialist - reviews first\",\n",
    "    instructions=\"\"\"You are the FIRST reviewer. Analyze the support response for:\n",
    "- Data exposure risks (customer IDs, case numbers that shouldn't be in emails)\n",
    "- PII handling concerns (names, order details)\n",
    "- Policy compliance issues\n",
    "\n",
    "Be concise. List only the security issues you find. Do NOT rewrite the email - just identify problems for later reviewers to address.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# 2nd: Accuracy reviewer - checks facts and promises\n",
    "accuracy_reviewer = ChatAgent(\n",
    "    name=\"AccuracyReviewer\", \n",
    "    description=\"Factual accuracy specialist - reviews second\",\n",
    "    instructions=\"\"\"You are the SECOND reviewer. Analyze the support response for:\n",
    "- Unrealistic promises or timelines\n",
    "- Unverifiable claims\n",
    "- Compensation appropriateness\n",
    "\n",
    "Consider the security feedback from the previous reviewer. Be concise. List only the accuracy issues. Do NOT rewrite the email - just identify problems for the final reviewer to address.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# 3rd: Tone reviewer - applies all feedback and produces final email\n",
    "tone_reviewer = ChatAgent(\n",
    "    name=\"ToneReviewer\",\n",
    "    description=\"Tone specialist and final editor - produces revised email\",\n",
    "    instructions=\"\"\"You are the FINAL reviewer. Your job is to:\n",
    "1. Consider ALL feedback from SecurityReviewer and AccuracyReviewer\n",
    "2. Review the tone and empathy of the original email\n",
    "3. **PRODUCE A FINAL REVISED EMAIL** that:\n",
    "   - Addresses security concerns (remove/mask sensitive identifiers if needed)\n",
    "   - Fixes accuracy issues (realistic timelines, appropriate promises)\n",
    "   - Maintains professional, empathetic tone\n",
    "   - Is ready to send to the customer\n",
    "\n",
    "End your response with the complete revised email in a clear format.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Three specialist reviewers defined:\")\n",
    "print(\"   1. SecurityReviewer - identifies security issues\")\n",
    "print(\"   2. AccuracyReviewer - checks facts and promises\")  \n",
    "print(\"   3. ToneReviewer - applies all feedback and produces FINAL email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27452e67",
   "metadata": {},
   "source": [
    "## Build Group Chat with Round-Robin\n",
    "\n",
    "Simple selection: each reviewer speaks in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4ab3019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Group chat built with round-robin selection\n",
      "   Order: SecurityReviewer ‚Üí AccuracyReviewer ‚Üí ToneReviewer (final)\n"
     ]
    }
   ],
   "source": [
    "# Sample draft response to review\n",
    "draft_to_review = \"\"\"\n",
    "Subject: Re: Order #12345 - Delivery Issue\n",
    "\n",
    "Dear Sarah,\n",
    "\n",
    "I'm so sorry to hear about the missing package! This must be incredibly frustrating.\n",
    "\n",
    "I've located your order and can confirm it was marked as delivered on Monday. Here's what I'll do:\n",
    "\n",
    "1. I've opened an investigation with our shipping partner (Case #INV-789)\n",
    "2. As a Premium customer, I'm expediting a replacement shipment TODAY\n",
    "3. The replacement will arrive by Thursday, well before your Friday presentation\n",
    "\n",
    "Your account has also been credited $50 for the inconvenience.\n",
    "\n",
    "If you need anything else, reply directly to this email - I'm here to help!\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "\"\"\"\n",
    "\n",
    "# Round-robin selector: each reviewer speaks in order\n",
    "def round_robin_selector(state: GroupChatState) -> str:\n",
    "    \"\"\"Pick the next speaker based on round index.\"\"\"\n",
    "    participants = list(state.participants.keys())\n",
    "    return participants[state.current_round % len(participants)]\n",
    "\n",
    "# Build group chat with round-robin selection\n",
    "# ORDER MATTERS: Security ‚Üí Accuracy ‚Üí Tone (final editor)\n",
    "review_group_chat = (\n",
    "    GroupChatBuilder()\n",
    "    .with_orchestrator(selection_func=round_robin_selector, orchestrator_name=\"RoundRobinOrchestrator\")\n",
    "    .participants([security_reviewer, accuracy_reviewer, tone_reviewer])  # Order: Security ‚Üí Accuracy ‚Üí Tone\n",
    "    .with_termination_condition(lambda msgs: len([m for m in msgs if m.role.value == \"assistant\"]) >= 3)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Group chat built with round-robin selection\")\n",
    "print(\"   Order: SecurityReviewer ‚Üí AccuracyReviewer ‚Üí ToneReviewer (final)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7588de",
   "metadata": {},
   "source": [
    "## Test Round-Robin Group Chat\n",
    "\n",
    "Each reviewer analyzes the draft in turn, building on previous insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4794a908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù DRAFT TO REVIEW:\n",
      "\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I'm so sorry to hear about the missing package! This must be incredibly frustrating.\n",
      "\n",
      "I've located your order and can confirm it was marked as delivered on Monday. Here's what I'll do:\n",
      "\n",
      "1. I've opened an investigation with our shipping partner (Case #INV-789)\n",
      "2. As a Premium customer, I'm expediting a replacement shipment TODAY\n",
      "3. The replacement will arrive by Thursday, well before your Friday presentation\n",
      "\n",
      "Your account has also been credited $50 for the inconvenience.\n",
      "\n",
      "If you need anything else, reply directly to this email - I'm here to help!\n",
      "\n",
      "Best regards,\n",
      "Support Team\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîÑ ROUND-ROBIN GROUP CHAT (each reviewer speaks in turn):\n",
      "\n",
      "\n",
      "ü§ñ [SecurityReviewer] (Turn #1): 1. **Data Exposure Risks**:\n",
      "   - Customer ID (actual order number #12345) disclosed in the email subject.\n",
      "   - Case number (INV-789) shared in the body of the email, which may be sensitive and not appropriate for external communication.\n",
      "\n",
      "2. **PII Handling Concerns**:\n",
      "   - Customer's name (Sarah) is mentioned, which is considered personally identifiable information (PII).\n",
      "   - Implicit mention of additional order details (delivery status, compensation amount) that could identify the customer.\n",
      "\n",
      "3. **Policy Compliance Issues**:\n",
      "   - Lack of encryption or secure communication method for sharing sensitive information, which may violate data protection policies.\n",
      "   - Potential violation of customer privacy policies by including details about the case and order in non-secure communication.\n",
      "\n",
      "\n",
      "ü§ñ [AccuracyReviewer] (Turn #2): 1. **Unrealistic Promises or Timelines**: \n",
      "   - The promise of expediting a replacement shipment \"TODAY\" and guaranteeing arrival by Thursday may not be feasible without confirming shipping conditions.\n",
      "\n",
      "2. **Unverifiable Claims**: \n",
      "   - The assertion that the replacement will arrive \"well before\" the Friday presentation is questionable without firm evidence, such as shipping confirmation or tracking.\n",
      "\n",
      "3. **Compensation Appropriateness**: \n",
      "   - The offered compensation of $50 may not align with the inconvenience caused by a missing package, particularly considering the urgency of the situation. The appropriateness of the amount is unclear and could be insufficient given the customer's presentation deadline.\n",
      "\n",
      "\n",
      "ü§ñ [ToneReviewer] (Turn #3): Subject: Re: Order - Delivery Issue\n",
      "\n",
      "Dear [Customer‚Äôs Name],\n",
      "\n",
      "Thank you for reaching out about your order. I understand that the issue with your missing package is frustrating, especially with your upcoming presentation. \n",
      "\n",
      "I've researched your order and see that it was marked as delivered on Monday. To assist you, here‚Äôs what I‚Äôm doing:\n",
      "\n",
      "1. I‚Äôve initiated an investigation with our shipping partner. Please note, they will provide updates on Case #INV-XXXX shortly.\n",
      "2. I‚Äôm arranging for a replacement shipment, which I will prioritize. While I cannot guarantee delivery by a specific date, I will do my best to expedite it as quickly as possible.\n",
      "3. Additionally, I have credited your account with $50 as an acknowledgment of the inconvenience you've experienced.\n",
      "\n",
      "If you have any further questions or need assistance, please don't hesitate to reply to this email. I‚Äôm here to help you.\n",
      "\n",
      "Thank you for your understanding.\n",
      "\n",
      "Best regards,  \n",
      "[Your Name]  \n",
      "Support Team\n",
      "\n",
      "============================================================\n",
      "üìä EXECUTION ORDER: SecurityReviewer ‚Üí AccuracyReviewer ‚Üí ToneReviewer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the group chat with round-robin selection\n",
    "from agent_framework._workflows._events import AgentRunUpdateEvent\n",
    "\n",
    "async def test_round_robin_group_chat():\n",
    "    print(\"üìù DRAFT TO REVIEW:\")\n",
    "    print(draft_to_review)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nüîÑ ROUND-ROBIN GROUP CHAT (each reviewer speaks in turn):\\n\")\n",
    "    \n",
    "    last_executor_id: str | None = None\n",
    "    agent_order = []\n",
    "    \n",
    "    async for event in review_group_chat.run_stream(f\"Review this support response:\\n{draft_to_review}\"):\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            eid = event.executor_id\n",
    "            if eid != last_executor_id:\n",
    "                if last_executor_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_order.append(eid)\n",
    "                print(f\"\\nü§ñ [{eid}] (Turn #{len(agent_order)}):\", end=\" \", flush=True)\n",
    "                last_executor_id = eid\n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(f\"üìä EXECUTION ORDER: {' ‚Üí '.join(agent_order)}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "await test_round_robin_group_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5350b545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù DRAFT TO REVIEW:\n",
      "\n",
      "Subject: Re: Order #12345 - Delivery Issue\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I'm so sorry to hear about the missing package! This must be incredibly frustrating.\n",
      "\n",
      "I've located your order and can confirm it was marked as delivered on Monday. Here's what I'll do:\n",
      "\n",
      "1. I've opened an investigation with our shipping partner (Case #INV-789)\n",
      "2. As a Premium customer, I'm expediting a replacement shipment TODAY\n",
      "3. The replacement will arrive by Thursday, well before your Friday presentation\n",
      "\n",
      "Your account has also been credited $50 for the inconvenience.\n",
      "\n",
      "If you need anything else, reply directly to this email - I'm here to help!\n",
      "\n",
      "Best regards,\n",
      "Support Team\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üß† AGENT-ORCHESTRATED GROUP CHAT (intelligent speaker selection):\n",
      "\n",
      "\n",
      "ü§ñ [SecurityReviewer] (Call #1): 1. Data exposure risks:\n",
      "   - Customer IDs included in the email may expose sensitive information.\n",
      "   - Case numbers mentioned that shouldn't be disseminated via email.\n",
      "\n",
      "2. PII handling concerns:\n",
      "   - Full names of customers present in the correspondence.\n",
      "   - Order details shared that could identify individual customers and their transactions.\n",
      "\n",
      "3. Policy compliance issues:\n",
      "   - Use of unencrypted email for sharing sensitive information potentially violates data protection policies.\n",
      "   - Lack of adherence to proper protocols for handling PII as specified in relevant regulations.\n",
      "\n",
      "\n",
      "ü§ñ [AccuracyReviewer] (Call #1): 1. Unrealistic promises or timelines:\n",
      "   - A timeline provided for issue resolution appears overly optimistic without sufficient context.\n",
      "\n",
      "2. Unverifiable claims:\n",
      "   - Claims about the effectiveness of implemented solutions lack evidence or data to support them.\n",
      "   - Assertions about \"industry-leading\" measures without backing up with specific examples or benchmarks.\n",
      "\n",
      "3. Compensation appropriateness:\n",
      "   - Compensation offered may not align with the severity of the reported issues and customer impact.\n",
      "\n",
      "\n",
      "ü§ñ [ToneReviewer] (Call #1): Subject: Update on Your Support Request\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "Thank you for your patience as we work to resolve the issues you've reported. We understand how important this matter is to you, and we appreciate the opportunity to assist.\n",
      "\n",
      "We want to assure you that we take your concerns seriously and are actively addressing them. Our team is implementing measures aimed at improving your experience, and we're currently in the process of testing these changes to ensure their effectiveness. While we are working diligently, these adjustments may take a bit longer than initially anticipated. We expect to have significant updates within the next [realistic timeline, e.g., \"two weeks\"], and we will keep you informed throughout this process.\n",
      "\n",
      "As a token of our appreciation for your understanding, we would like to offer you a [considered compensation, e.g., \"discount on your next purchase,\" or \"extended service period\"], which we believe will help enhance your overall experience.\n",
      "\n",
      "Please rest assured that your privacy and security are our top priorities. We are committed to adhering to all relevant data protection regulations and will ensure that sensitive information is handled appropriately moving forward.\n",
      "\n",
      "If you have any further questions or concerns, please do not hesitate to reach out.\n",
      "\n",
      "Thank you for your understanding and continued support.\n",
      "\n",
      "Warm regards,\n",
      "\n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "[Your Company]  \n",
      "[Your Contact Information]  \n",
      "\n",
      "============================================================\n",
      "üìä EXECUTION SUMMARY\n",
      "============================================================\n",
      "   Total calls: 3\n",
      "\n",
      "   Calls per agent:\n",
      "      AccuracyReviewer: 1 call(s)\n",
      "      SecurityReviewer: 1 call(s)\n",
      "      ToneReviewer: 1 call(s)\n",
      "\n",
      "   üí° The orchestrator dynamically selected speakers\n",
      "      based on what was needed at each step\n",
      "\n",
      "============================================================\n",
      "üìß FINAL REVISED EMAIL (from ToneReviewer)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Agent-based orchestrator for intelligent speaker selection\n",
    "from typing import cast\n",
    "from agent_framework._workflows._events import AgentRunUpdateEvent, WorkflowOutputEvent\n",
    "from agent_framework._types import ChatMessage\n",
    "\n",
    "orchestrator_agent = ChatAgent(\n",
    "    name=\"ReviewOrchestrator\",\n",
    "    description=\"Coordinates multi-agent review process\",\n",
    "    instructions=f\"\"\"You coordinate a team reviewing this support response:\n",
    "\n",
    "{draft_to_review}\n",
    "\n",
    "YOUR TEAM:\n",
    "- SecurityReviewer: Identifies security/PII issues (reviews first)\n",
    "- AccuracyReviewer: Checks facts and promises (reviews second)\n",
    "- ToneReviewer: Final editor who produces the revised email (reviews last)\n",
    "\n",
    "YOUR PROCESS:\n",
    "1. Start with SecurityReviewer to check data safety and PII\n",
    "2. Then AccuracyReviewer to verify claims and timelines\n",
    "3. **Finally, ToneReviewer to produce the FINAL REVISED EMAIL** incorporating all feedback\n",
    "4. If needed, you may ask follow-up questions to any reviewer\n",
    "5. End when ToneReviewer delivers the complete revised email\n",
    "\n",
    "Select speakers intelligently. CRITICAL: ToneReviewer must go last and produce the final email.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Build group chat with agent-based orchestration\n",
    "# ORDER: Security ‚Üí Accuracy ‚Üí Tone (final editor)\n",
    "intelligent_review_chat = (\n",
    "    GroupChatBuilder()\n",
    "    .with_orchestrator(agent=orchestrator_agent)\n",
    "    .participants([security_reviewer, accuracy_reviewer, tone_reviewer])\n",
    "    .with_termination_condition(lambda msgs: len([m for m in msgs if m.role.value == \"assistant\"]) >= 5)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run with detailed logging\n",
    "async def test_agent_orchestrated_group_chat():\n",
    "    print(\"üìù DRAFT TO REVIEW:\")\n",
    "    print(draft_to_review)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nüß† AGENT-ORCHESTRATED GROUP CHAT (intelligent speaker selection):\\n\")\n",
    "    \n",
    "    last_executor_id: str | None = None\n",
    "    agent_calls: dict[str, int] = {}\n",
    "    \n",
    "    async for event in intelligent_review_chat.run_stream(\"Review this support response. Security and Accuracy reviewers identify issues, then ToneReviewer produces the final revised email.\"):\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            eid = event.executor_id\n",
    "            if eid != last_executor_id:\n",
    "                if last_executor_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_calls[eid] = agent_calls.get(eid, 0) + 1\n",
    "                print(f\"\\nü§ñ [{eid}] (Call #{agent_calls[eid]}):\", end=\" \", flush=True)\n",
    "                last_executor_id = eid\n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            output_messages = cast(list[ChatMessage], event.data)\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(\"üìä EXECUTION SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"   Total calls: {sum(agent_calls.values())}\")\n",
    "            print(\"\\n   Calls per agent:\")\n",
    "            for agent, count in sorted(agent_calls.items()):\n",
    "                print(f\"      {agent}: {count} call(s)\")\n",
    "            \n",
    "            print(\"\\n   üí° The orchestrator dynamically selected speakers\")\n",
    "            print(\"      based on what was needed at each step\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"üìß FINAL REVISED EMAIL (from ToneReviewer)\")\n",
    "            print(\"=\" * 60)\n",
    "            for msg in reversed(output_messages):\n",
    "                if msg.role.value == \"assistant\" and \"ToneReviewer\" in str(msg):\n",
    "                    print(msg.text)\n",
    "                    break\n",
    "\n",
    "await test_agent_orchestrated_group_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e052",
   "metadata": {},
   "source": [
    "# 12. Magentic Orchestration\n",
    "\n",
    "![Magentic Pattern](images/magentic-workflow.png)\n",
    "\n",
    "Magentic is the most powerful orchestration pattern - a manager dynamically plans and delegates to specialists based on evolving task requirements.\n",
    "\n",
    "**When to Use:**\n",
    "- Complex, open-ended tasks requiring multiple iterations\n",
    "- Tasks where the solution path isn't known in advance\n",
    "- Research + analysis workflows with code execution\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Simple linear pipelines (use Sequential)\n",
    "- Fixed review rounds (use Group Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0d883",
   "metadata": {},
   "source": [
    "## Use Case: Market Research Report\n",
    "\n",
    "A complex task requiring:\n",
    "1. **Research Agent** - Web search for current data\n",
    "2. **Analyst Agent** - Code execution for data processing\n",
    "3. **Manager** - Dynamic planning and synthesis\n",
    "\n",
    "The manager autonomously decides which agent to call and when based on progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f64fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Magentic agents defined: ResearcherAgent, AnalystAgent, ResearchManager\n"
     ]
    }
   ],
   "source": [
    "# Magentic Orchestration: Research + Analysis workflow\n",
    "import json\n",
    "from typing import cast\n",
    "from agent_framework import (\n",
    "    AgentRunUpdateEvent,\n",
    "    MagenticOrchestratorEvent,\n",
    "    MagenticProgressLedger,\n",
    ")\n",
    "\n",
    "# Research Agent - uses web search capability\n",
    "# Note: In production, use OpenAI's gpt-4o-search-preview or add web search tools\n",
    "researcher_agent = ChatAgent(\n",
    "    name=\"ResearcherAgent\",\n",
    "    description=\"Specialist in research and information gathering about markets, trends, and data\",\n",
    "    instructions=\"\"\"You are a Research Specialist. Your job is to:\n",
    "- Gather factual information about topics\n",
    "- Find current statistics and trends\n",
    "- Provide sources for your findings\n",
    "\n",
    "When asked about market data, provide realistic example data with citations.\n",
    "Be concise and factual. Format data clearly for analysis.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Analyst Agent - processes and analyzes data\n",
    "# Note: In production, add HostedCodeInterpreterTool for real code execution\n",
    "analyst_agent = ChatAgent(\n",
    "    name=\"AnalystAgent\",\n",
    "    description=\"Data analyst who processes information and creates insights with calculations\",\n",
    "    instructions=\"\"\"You are a Data Analyst. Your job is to:\n",
    "- Process and analyze data provided by the researcher\n",
    "- Perform calculations (growth rates, comparisons, projections)\n",
    "- Create clear tables and visualizations descriptions\n",
    "- Identify trends and insights\n",
    "\n",
    "Show your calculations step by step. Format results in clear tables.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Manager Agent - orchestrates the research workflow\n",
    "manager_agent = ChatAgent(\n",
    "    name=\"ResearchManager\",\n",
    "    description=\"Orchestrator that coordinates research and analysis workflows\",\n",
    "    instructions=\"\"\"You manage a research team to complete comprehensive analysis tasks.\n",
    "\n",
    "YOUR TEAM:\n",
    "- ResearcherAgent: Gathers information, statistics, and market data\n",
    "- AnalystAgent: Processes data, performs calculations, creates insights\n",
    "\n",
    "YOUR PROCESS:\n",
    "1. Break down the research request into subtasks\n",
    "2. Delegate to ResearcherAgent to gather relevant data\n",
    "3. Delegate to AnalystAgent to process and analyze the data\n",
    "4. Continue iterating until you have comprehensive insights\n",
    "5. Synthesize all findings into a final report\n",
    "\n",
    "You dynamically decide who to call based on what's needed. You may call agents multiple times.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Magentic agents defined: ResearcherAgent, AnalystAgent, ResearchManager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ddaa3",
   "metadata": {},
   "source": [
    "## Build & Run Magentic Workflow\n",
    "\n",
    "The manager dynamically plans and delegates. Watch how it calls different agents based on the evolving task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "919845da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ MAGENTIC RESEARCH WORKFLOW\n",
      "============================================================\n",
      "üìã TASK:\n",
      "\n",
      "Analyze the global electric vehicle (EV) market:\n",
      "1. Find the top 5 EV manufacturers by market share\n",
      "2. Calculate year-over-year growth rates\n",
      "3. Compare EV adoption rates in US, Europe, and China\n",
      "4. Provide a summary table and key insights\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üìã ORCHESTRATOR: PLAN_CREATED\n",
      "=======================================================\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üìã ORCHESTRATOR: PROGRESS_LEDGER_UPDATED\n",
      "=======================================================\n",
      "   ‚û°Ô∏è Next: ResearcherAgent\n",
      "   üí≠ Why: To proceed with the first step, ResearcherAgent needs to begin gathering the market share data....\n",
      "\n",
      "ü§ñ [ResearcherAgent] (Call #1): As of 2023, the market share data for the top electric vehicle (EV) manufacturers is as follows:\n",
      "\n",
      "1. **Tesla**: 19% \n",
      "   - Tesla continues to dominate the EV market with significant sales figures, particularly in the United States and China.\n",
      "\n",
      "2. **BYD**: 15%\n",
      "   - BYD has increased its market share rapidly, particularly in China, where it has a strong presence in both battery electric vehicles (BEVs) and plug-in hybrid electric vehicles (PHEVs).\n",
      "\n",
      "3. **Volkswagen Group**: 11%\n",
      "   - The Volkswagen Group, which includes brands like Audi and Porsche, has seen growing investments in EVs and launched several new models in 2023.\n",
      "\n",
      "4. **SAIC Motor**: 9%\n",
      "   - SAIC continues to lead in the Chinese market with its extensive offerings of electric vehicles, particularly through its MG and Roewe brands.\n",
      "\n",
      "5. **Ford**: 7%\n",
      "   - Ford has made significant investments in electrification, with the F-150 Lightning and Mustang Mach-E contributing to its market presence in North America.\n",
      "\n",
      "### Summary Table\n",
      "\n",
      "| Manufacturer       | Market Share (%) |\n",
      "|--------------------|------------------|\n",
      "| Tesla              | 19               |\n",
      "| BYD                | 15               |\n",
      "| Volkswagen Group   | 11               |\n",
      "| SAIC Motor         | 9                |\n",
      "| Ford               | 7                |\n",
      "\n",
      "### Sources\n",
      "- EV Sales data (2023). InsideEVs. Retrieved from: [insideevs.com](https://insideevs.com/news/586132/global-ev-sales-q2-2023/)\n",
      "- Market analysis report (2023). Statista. Retrieved from: [statista.com](https://www.statista.com/statistics/1181207/global-electric-vehicle-market-share-by-manufacturer/) \n",
      "\n",
      "Note: Market shares are approximate and can vary based on specific data collection times and methodologies used by different reporting agencies.\n",
      "\n",
      "=======================================================\n",
      "üìã ORCHESTRATOR: PROGRESS_LEDGER_UPDATED\n",
      "=======================================================\n",
      "   ‚û°Ô∏è Next: ResearcherAgent\n",
      "   üí≠ Why: The next step requires gathering historical sales data for the year-over-year growth rate calculatio...\n",
      "\n",
      "\n",
      "\n",
      "ü§ñ [ResearcherAgent] (Call #2): Here is the global electric vehicle (EV) sales/registration data for the past few years, which will allow calculation of year-over-year growth rates:\n",
      "\n",
      "### Global Electric Vehicle Sales/Registration Data\n",
      "\n",
      "| Year | Global EV Sales (Units) | Year-over-Year Growth (%) |\n",
      "|------|-------------------------|---------------------------|\n",
      "| 2020 | 3,241,000               | -                         |\n",
      "| 2021 | 6,751,000               | 108%                      |\n",
      "| 2022 | 10,496,000              | 55%                       |\n",
      "| 2023 | 14,182,000              | 35%                       |\n",
      "\n",
      "### Summary of Year-over-Year Growth Calculation:\n",
      "- **2020 to 2021**: ((6,751,000 - 3,241,000) / 3,241,000) * 100 = 108%\n",
      "- **2021 to 2022**: ((10,496,000 - 6,751,000) / 6,751,000) * 100 = 55%\n",
      "- **2022 to 2023**: ((14,182,000 - 10,496,000) / 10,496,000) * 100 = 35%\n",
      "\n",
      "### Sources\n",
      "- EV Sales Data (2023). International Energy Agency (IEA). Retrieved from: [iea.org](https://www.iea.org/reports/global-ev-outlook-2023)\n",
      "- Market trend analysis (2023). BloombergNEF. Retrieved from: [bnef.com](https://about.bnef.com/research/global-ev-outlook-2023/)\n",
      "\n",
      "This data provides a clear picture of the upward trend in electric vehicle sales globally over the past few years.\n",
      "\n",
      "=======================================================\n",
      "üìã ORCHESTRATOR: PROGRESS_LEDGER_UPDATED\n",
      "=======================================================\n",
      "   ‚û°Ô∏è Next: ResearcherAgent\n",
      "   üí≠ Why: We need to address the third part of the request concerning EV adoption rates....\n",
      "\n",
      "\n",
      "\n",
      "ü§ñ [ResearcherAgent] (Call #3): As of 2023, here are the current electric vehicle (EV) adoption rates in the US, Europe, and China, along with relevant trends:\n",
      "\n",
      "### Current EV Adoption Rates\n",
      "\n",
      "| Region      | EV Adoption Rate (%) | Relevant Trends                                     |\n",
      "|-------------|---------------------|----------------------------------------------------|\n",
      "| **United States**  | 6.0%                 | The US EV market is growing rapidly due to federal incentives and increased availability of models. Sales surged in 2023, particularly in the SUV and pickup segments. |\n",
      "| **Europe**         | 15.0%                | Europe leads in EV uptake, propelled by stringent emissions regulations and extensive charging infrastructure. The growth is driven largely by countries like Norway, where adoption exceeds 45%. |\n",
      "| **China**          | 20.0%                | China remains the world's largest EV market, with rapid growth in EV sales driven by government subsidies and a strong domestic manufacturing base. Major brands like BYD and Tesla continue to lead. |\n",
      "\n",
      "### Trends:\n",
      "- **United States**: Federal incentives, such as the Inflation Reduction Act, are encouraging manufacturers to invest in EV production, contributing to a significant increase in adoption.\n",
      "  \n",
      "- **Europe**: The European Union's commitment to reducing carbon emissions is pushing automakers to increase EV offerings. The rise of charging infrastructure has also made EVs more accessible to consumers.\n",
      "\n",
      "- **China**: Government policies supporting electrification, along with a growing consumer preference for green technology, are key factors in the impressive adoption rate. Chinese manufacturers are expected to maintain their dominance due to innovative battery technologies and lower price points.\n",
      "\n",
      "### Sources\n",
      "- EV Adoption Rates (2023). IEA - Global EV Outlook. Retrieved from: [iea.org](https://www.iea.org/reports/global-ev-outlook-2023)\n",
      "- Market Trends (2023). BloombergNEF Electric Vehicle Outlook. Retrieved from: [bnef.com](https://about.bnef.com/research/global-ev-outlook-2023)\n",
      "\n",
      "This data illustrates the growing acceptance of electric vehicles across these major markets, driven by various regional policies and consumer trends.\n",
      "\n",
      "=======================================================\n",
      "üìã ORCHESTRATOR: PROGRESS_LEDGER_UPDATED\n",
      "=======================================================\n",
      "   ‚û°Ô∏è Next: AnalystAgent\n",
      "   üí≠ Why: Next, we need to analyze the collected data and generate insights based on the market overview of EV...\n",
      "\n",
      "\n",
      "\n",
      "ü§ñ [AnalystAgent] (Call #1): Here's a compiled summary table that includes the data on the top five EV manufacturers, year-over-year growth rates of global EV sales, and EV adoption rates in the US, Europe, and China:\n",
      "\n",
      "### Summary Table of Electric Vehicle Data\n",
      "\n",
      "| Category                     | Manufacturer/Region     | Value/Rate                  |\n",
      "|-----------------------------|-------------------------|-----------------------------|\n",
      "| **Top EV Manufacturers**     | Tesla                   | 19% Market Share            |\n",
      "|                             | BYD                     | 15% Market Share            |\n",
      "|                             | Volkswagen Group        | 11% Market Share            |\n",
      "|                             | SAIC Motor              | 9% Market Share             |\n",
      "|                             | Ford                    | 7% Market Share             |\n",
      "| **Global EV Year-over-Year Growth Rates** | 2020 to 2021   | 108%                        |\n",
      "|                             | 2021 to 2022            | 55%                         |\n",
      "|                             | 2022 to 2023            | 35%                         |\n",
      "| **EV Adoption Rates**       | United States           | 6.0%                        |\n",
      "|                             | Europe                  | 15.0%                       |\n",
      "|                             | China                   | 20.0%                       |\n",
      "\n",
      "### Key Insights Regarding the Global Electric Vehicle Market:\n",
      "\n",
      "1. **Dominance of Leading Manufacturers**:\n",
      "   - Tesla continues to lead the market with a 19% share, benefitting from strong brand recognition, extensive charging networks, and loyal customer base. BYD's significant growth to 15% indicates its successful penetration into both battery-electric and hybrid segments, particularly in China.\n",
      "\n",
      "2. **Rapid Growth in Global EV Sales**:\n",
      "   - The explosive growth rates from 2020 to 2021 (108%) demonstrate the market's burgeoning acceptance and enthusiasm for EVs, largely fueled by increased model availability and consumer incentives. While growth rates have slowed to 35% in 2023, they remain robust, indicating a sustainable upward trend.\n",
      "\n",
      "3. **Diverse Regional Adoption**:\n",
      "   - The highest adoption rate is in China at 20%, showcasing the effectiveness of government support and local manufacturing capabilities. Europe‚Äôs 15% adoption rate reflects strong governmental policies aimed at reducing emissions, while the US trails behind at 6.0%, highlighting the need for increased incentives and infrastructure development.\n",
      "\n",
      "4. **Future Potential and Market Opportunities**:\n",
      "   - As the EV market expands, there‚Äôs clear potential for all manufacturers to increase their market share. Companies like Ford are still in the early stages of electrification efforts but are investing heavily in new models, presenting opportunities for future growth as EVs become more mainstream.\n",
      "\n",
      "5. **Sustainability and Environmental Goals**:\n",
      "   - The overall trend toward increased EV adoption across various regions supports global sustainability goals. As governments promote cleaner transportation alternatives, the market is likely to grow, necessitating rapid advancements in charging infrastructure and battery technology.\n",
      "\n",
      "This compiled data and analysis highlight the rapid evolution of the EV market and provide a clear view of the competitive landscape, as well as regional differences in adoption and growth. As we advance, continual monitoring of trends will be crucial for stakeholders across the automotive sector.\n",
      "\n",
      "=======================================================\n",
      "üìã ORCHESTRATOR: PROGRESS_LEDGER_UPDATED\n",
      "=======================================================\n",
      "   ‚û°Ô∏è Next: AnalystAgent\n",
      "   üí≠ Why: The analysis has been completed, but further inquiries or clarifications may be needed regarding the...\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä EXECUTION SUMMARY\n",
      "============================================================\n",
      "   Total agent calls: 4\n",
      "\n",
      "   Calls per agent:\n",
      "      AnalystAgent: 1 call(s)\n",
      "      ResearcherAgent: 3 call(s)\n",
      "\n",
      "   ‚ú® Manager dynamically orchestrated:\n",
      "      - Broke down complex task into subtasks\n",
      "      - Called ResearcherAgent for data gathering\n",
      "      - Called AnalystAgent for processing\n",
      "      - Synthesized into final report\n",
      "\n",
      "============================================================\n",
      "üìë FINAL RESEARCH REPORT\n",
      "============================================================\n",
      "Here's a comprehensive analysis of the global electric vehicle (EV) market based on the latest data available:\n",
      "\n",
      "1. **Top 5 EV Manufacturers by Market Share**:\n",
      "   - **Tesla**: 19%\n",
      "   - **BYD**: 15%\n",
      "   - **Volkswagen Group**: 11%\n",
      "   - **SAIC Motor**: 9%\n",
      "   - **Ford**: 7%\n",
      "\n",
      "2. **Year-over-Year Growth Rates**:\n",
      "   - The global electric vehicle market has seen remarkable growth over the past few years:\n",
      "     - **2020 to 2021**: 108%\n",
      "     - **2021 to 2022**: 55%\n",
      "     - **2022 to 2023**: 35%\n",
      "   This demonstrates a strong upward trend, particularly in the early pandemic recovery phase, followed by sustained growth.\n",
      "\n",
      "3. **EV Adoption Rates**:\n",
      "   - The current adoption rates for electric vehicles are as follows:\n",
      "     - **United States**: 6.0%\n",
      "     - **Europe**: 15.0%\n",
      "     - **China**: 20.0%\n",
      "   This reflects significant regional differences, with China leading in adoption largely due to government support and local manufacturing.\n",
      "\n",
      "### Summary Table\n",
      "\n",
      "| Category                     | Manufacturer/Region     | Value/Rate                  |\n",
      "|-----------------------------|-------------------------|-----------------------------|\n",
      "| **Top EV Manufacturers**     | Tesla                   | 19% Market Share            |\n",
      "|                             | BYD                     | 15% Market Share            |\n",
      "|                             | Volkswagen Group        | 11% Market Share            |\n",
      "|                             | SAIC Motor              | 9% Market Share             |\n",
      "|                             | Ford                    | 7% Market Share             |\n",
      "| **Global EV Year-over-Year Growth Rates** | 2020 to 2021   | 108%                        |\n",
      "|                             | 2021 to 2022            | 55%                         |\n",
      "|                             | 2022 to 2023            | 35%                         |\n",
      "| **EV Adoption Rates**       | United States           | 6.0%                        |\n",
      "|                             | Europe                  | 15.0%                       |\n",
      "|                             | China                   | 20.0%                       |\n",
      "\n",
      "### Key Insights:\n",
      "- **Market Leadership**: Tesla maintains a strong lead in the EV market, but BYD is rapidly gaining ground, particularly in its home market of China. Volkswagen, with a strong lineup of new models, is also making significant moves.\n",
      "  \n",
      "- **Sustainable Growth**: The global EV market is on a sustainable growth trajectory, highlighted by impressive year-over-year growth rates, especially following the pandemic.\n",
      "\n",
      "- **Regional Variance**: There's a clear disparity in EV adoption rates, with China significantly ahead of both the US and Europe. Government incentives and robust infrastructure play a crucial role in this.\n",
      "\n",
      "- **Future Outlook**: Opportunities exist for manufacturers to increase their market shares as consumer acceptance grows and as policies become more favorable toward EV adoption. The potential for technological advancements in battery and charging infrastructure will likely further facilitate this growth.\n",
      "\n",
      "This analysis provides a detailed overview of the current state and future outlook of the global electric vehicle market, highlighting both the competitive landscape and regional dynamics. If you have any further questions or need more specific insights, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Build Magentic workflow\n",
    "magentic_research_workflow = (\n",
    "    MagenticBuilder()\n",
    "    .participants([researcher_agent, analyst_agent])\n",
    "    .with_manager(\n",
    "        agent=manager_agent,\n",
    "        max_round_count=10,  # Maximum delegation rounds\n",
    "        max_stall_count=2,   # Replan after 2 stalls\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Research task - complex enough to require multiple agent interactions\n",
    "research_task = \"\"\"\n",
    "Analyze the global electric vehicle (EV) market:\n",
    "1. Find the top 5 EV manufacturers by market share\n",
    "2. Calculate year-over-year growth rates\n",
    "3. Compare EV adoption rates in US, Europe, and China\n",
    "4. Provide a summary table and key insights\n",
    "\"\"\"\n",
    "\n",
    "async def run_magentic_research():\n",
    "    print(\"üî¨ MAGENTIC RESEARCH WORKFLOW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìã TASK:\\n{research_task}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    last_message_id: str | None = None\n",
    "    agent_calls: dict[str, int] = {}\n",
    "    \n",
    "    async for event in magentic_research_workflow.run_stream(research_task):\n",
    "        # Track streaming from agents\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            message_id = event.data.message_id\n",
    "            executor_id = event.executor_id\n",
    "            \n",
    "            if message_id != last_message_id:\n",
    "                if last_message_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_calls[executor_id] = agent_calls.get(executor_id, 0) + 1\n",
    "                print(f\"\\nü§ñ [{executor_id}] (Call #{agent_calls[executor_id]}):\", end=\" \", flush=True)\n",
    "                last_message_id = message_id\n",
    "            \n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        # Track orchestration events\n",
    "        elif isinstance(event, MagenticOrchestratorEvent):\n",
    "            print(f\"\\n\\n{'='*55}\")\n",
    "            print(f\"üìã ORCHESTRATOR: {event.event_type.name}\")\n",
    "            print(f\"{'='*55}\")\n",
    "            \n",
    "            if isinstance(event.data, MagenticProgressLedger):\n",
    "                ledger = event.data.to_dict()\n",
    "                if \"next_speaker\" in ledger:\n",
    "                    next_info = ledger.get('next_speaker', {})\n",
    "                    if isinstance(next_info, dict):\n",
    "                        print(f\"   ‚û°Ô∏è Next: {next_info.get('answer', 'N/A')}\")\n",
    "                        reason = next_info.get('reason', '')\n",
    "                        if reason:\n",
    "                            print(f\"   üí≠ Why: {reason[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"   ‚û°Ô∏è Next: {next_info}\")\n",
    "        \n",
    "        # Final output\n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            output_messages = cast(list[ChatMessage], event.data)\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(\"üìä EXECUTION SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"   Total agent calls: {sum(agent_calls.values())}\")\n",
    "            print(\"\\n   Calls per agent:\")\n",
    "            for agent, count in sorted(agent_calls.items()):\n",
    "                print(f\"      {agent}: {count} call(s)\")\n",
    "            \n",
    "            print(\"\\n   ‚ú® Manager dynamically orchestrated:\")\n",
    "            print(f\"      - Broke down complex task into subtasks\")\n",
    "            print(f\"      - Called ResearcherAgent for data gathering\")\n",
    "            print(f\"      - Called AnalystAgent for processing\")\n",
    "            print(f\"      - Synthesized into final report\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"üìë FINAL RESEARCH REPORT\")\n",
    "            print(\"=\" * 60)\n",
    "            for msg in reversed(output_messages):\n",
    "                if msg.role.value == \"assistant\":\n",
    "                    print(msg.text)\n",
    "                    break\n",
    "\n",
    "await run_magentic_research()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
